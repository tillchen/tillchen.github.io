<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on Tianyao Chen</title>
    <link>https://tillchen.com/tags/AI/</link>
    <description>Recent content in AI on Tianyao Chen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 01 May 2020 09:25:01 +0200</lastBuildDate>
    
	<atom:link href="https://tillchen.com/tags/AI/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Machine Learning Notes</title>
      <link>https://tillchen.com/posts/2020-05-01-Machine-Learning-Notes/</link>
      <pubDate>Fri, 01 May 2020 09:25:01 +0200</pubDate>
      
      <guid>https://tillchen.com/posts/2020-05-01-Machine-Learning-Notes/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-learning&#34;&gt;Types of Learning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#supervised-learning&#34;&gt;Supervised learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#unsupervised-learning&#34;&gt;Unsupervised learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#semi-supervised-learning&#34;&gt;Semi-supervised learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reinforcement-learning&#34;&gt;Reinforcement learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#notations-and-definitions&#34;&gt;Notations and Definitions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fundamental-algorithms&#34;&gt;Fundamental Algorithms&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#linear-regression&#34;&gt;Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression&#34;&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#decision-tree-learning&#34;&gt;Decision Tree Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#support-vector-machine&#34;&gt;Support Vector Machine&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#dealing-with-noise&#34;&gt;Dealing With Noise&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#dealing-with-inherent-non-linearity-kernels&#34;&gt;Dealing with Inherent Non-Linearity (Kernels)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-nearest-neighbors-knn&#34;&gt;k-Nearest Neighbors (kNN)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#anatomy-of-a-learning-algorithm&#34;&gt;Anatomy of a Learning Algorithm&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#building-blocks&#34;&gt;Building blocks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gradient-descent&#34;&gt;Gradient Descent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#particularities&#34;&gt;Particularities&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This post is about some basic machine learning concepts.&lt;/p&gt;
&lt;h3 id=&#34;types-of-learning&#34;&gt;Types of Learning&lt;/h3&gt;
&lt;h4 id=&#34;supervised-learning&#34;&gt;Supervised learning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;In a feature vector $x_i$ (i=1&amp;hellip;N), a single feature is denoted by $x^j$ (j=1&amp;hellip;D). And $y_i$ is the label.&lt;/li&gt;
&lt;li&gt;The goal is to produce a model that takes a feature vector x as input and outputs the label.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;unsupervised-learning&#34;&gt;Unsupervised learning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The goal is either to transform the input feature vector into another vector (dimensionality reduction) or into a value (clustering, outlier detection).&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;semi-supervised-learning&#34;&gt;Semi-supervised learning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The dataset has both labeled and unlabeled (much higher quantity) data.&lt;/li&gt;
&lt;li&gt;The same goal as supervised learning.&lt;/li&gt;
&lt;li&gt;The hope is that the unlabeled data can help produce a better model. This is because a larger sample reflects better the probability distribution of the source of the labeled data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;reinforcement-learning&#34;&gt;Reinforcement learning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;It perceives the &lt;strong&gt;state&lt;/strong&gt; of the environment as a feature vector and then execute &lt;strong&gt;actions&lt;/strong&gt; in every state, which bring &lt;strong&gt;rewards&lt;/strong&gt; and move the machine to another state.&lt;/li&gt;
&lt;li&gt;The goal is to learn a &lt;strong&gt;policy&lt;/strong&gt;: a function that takes the feature vector as input and outputs an optimal action that maximizes the expected average reward.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s suited for problems whose decision making is sequential and the goal is long-term.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;notations-and-definitions&#34;&gt;Notations and Definitions&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$x_{l,u}^{(j)}$ in a neural net means the feature j of unit u in layer l.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$x$ is by default a column vector and $x^T$ a row vector. Then vector is on the left side of a matrix, we usually do $x^TA$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$arg max_{a \in A}f(a)$ returns the element a of the set A that maximizes f(a).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\mathbb{E}[\hat\theta(S_X) = \theta]$, where $\hat\theta(S_X)$ is the unbiased estimator of some statistic $\theta$ (e.g. $\mu$) calculated using a sample $S_X$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A hyperparameter is a parameter whose value is set before the learning process begins. By contrast, the values of other parameters are derived via training.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Model-based learning vs instance-based learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model-based: creates a model with parameters learned from the data. (Most supervised learning algorithms)&lt;/li&gt;
&lt;li&gt;Instance-based: uses the whole dataset as the model. Instead of performing explicit generalization, it compares new problem instances with instances seen in training (e.g. kNN).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Shallow vs deep learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shallow learning: learns the parameters of the model directly from the features of the training examples.&lt;/li&gt;
&lt;li&gt;Deep (neural network) learning: uses neural nets with more than one layer and learns the parameters from the outputs of the preceding layers instead.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;fundamental-algorithms&#34;&gt;Fundamental Algorithms&lt;/h2&gt;
&lt;h3 id=&#34;linear-regression&#34;&gt;Linear Regression&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;We want a model as a linear combination of features of example x ($w$ is a D-dimensional vector of parameters and b is a real number.):&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$f_{w,b}(x) = wx+b$$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;We find the optimal values $w^{*}$ and $b^{*}$ by minimizing the cost function below (the empirical risk - the average loss):&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\frac{1}{N}\sum_{i=1&amp;hellip;N}(f_{w,b}(x_i)-y_i)^2$$&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;Overfitting: a model that predicts well for the training data but not for data unseen during training. Linear regression rarely overfits.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The squared loss is used since it&amp;rsquo;s convinent. The absolute loss function doesn&amp;rsquo;t have a continuous derivative, which makes it not smooth. Unsmooth functions create difficulties for optimization problems in linear algebra.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To find the extrema (minima or maxima), we can simply set the gradient to zero.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;logistic-regression&#34;&gt;Logistic Regression&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Logistic regression is not a regression, but a classification learning algorithm. The names is because the mathematical formulation is similar to linear regression.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The standard logistic function or the sigmoid function:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$f(x)=\frac{1}{1+e^{-x}}$$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://tillchen.com/images/logistic_function.png&#34; alt=&#34;Logistic Function&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;The logistic regression model:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$f(x)=\frac{1}{1+e^{-(wx+b)}}$$&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;
&lt;p&gt;For binary classification, we can say the label is positive if the probability is higher than or equal to 0.5.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We optimize by using the maximum likelihood (the goodness of fit) ($y_i$ is either 0 or 1):&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$L_{w,b}=\prod_{i=1&amp;hellip;N}f_{w,b}(x_i)^{y_i}(1-f_{w,b}(x_i))^{1-y_i}$$&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;In practice, it&amp;rsquo;s more convenient to maximize the log-likelihood (since ln is a strictly increasing function, maximizing this is the same as maximizing its argument):&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$LogL_{w,b}=ln(L_{w,b})=\sum_{i=1}^Ny_iln(f_{w,b}(x_i)) + (1-y_i)ln(1-f_{w,b}(x_i))$$&lt;/p&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;Unlike linear regression, there&amp;rsquo;s no closed form solution. A typical numerical optimization procedure is gradient descent.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;decision-tree-learning&#34;&gt;Decision Tree Learning&lt;/h3&gt;
&lt;p&gt;To be filled.&lt;/p&gt;
&lt;h3 id=&#34;support-vector-machine&#34;&gt;Support Vector Machine&lt;/h3&gt;
&lt;p&gt;To be filled.&lt;/p&gt;
&lt;h4 id=&#34;dealing-with-noise&#34;&gt;Dealing With Noise&lt;/h4&gt;
&lt;p&gt;To be filled.&lt;/p&gt;
&lt;h5 id=&#34;dealing-with-inherent-non-linearity-kernels&#34;&gt;Dealing with Inherent Non-Linearity (Kernels)&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The kernel trick: use a function to &lt;em&gt;implicitly&lt;/em&gt; transform the original space into a higher dimensional space during the cost function optimization. (We hope that data will be linearly separable in the transformed space).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We use kernel functions or simply kernels to efficiently work in higher-dimensional spaces without doing this transformation explicitly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;By using the kernel trick, we get rid of a costly transformation to the higher dimension and avoid computing their dot-product.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We can use the quadratic kernel $k(x_i, x_k) = (x_ix_k)^2$ here. Another widely used kernel is the RBF (radial basis function) kernel: $k(x, x^{&#39;}) = exp(-\frac{||x-x^{&#39;}||^2}{2\sigma^2})$. It has infinite dimensions. We can vary the hyperparameter $\sigma$  and choose wether to get a smooth of curvy decision boundary.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;k-nearest-neighbors-knn&#34;&gt;k-Nearest Neighbors (kNN)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;kNN is a non-parametric learning algorithm. Contrary to other learning algorithms that allow discarding the training data after the model is built, kNN keeps all training examples in memory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The closeness of two examples is given by a distance function. Other than the Euclidean distance, we could also use the cosine similarity function ($0^{\circ}: 1; 90^{\circ}: 0; 180^{\circ}: -1$).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;anatomy-of-a-learning-algorithm&#34;&gt;Anatomy of a Learning Algorithm&lt;/h2&gt;
&lt;h3 id=&#34;building-blocks&#34;&gt;Building blocks&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;A loss function&lt;/li&gt;
&lt;li&gt;An optimization criterion based on the loss function (e.g. a cost function)&lt;/li&gt;
&lt;li&gt;An optimization routine leveraging training data to find a solution to the optimization criterion.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;gradient-descent&#34;&gt;Gradient Descent&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To find a local minimum, start at some random point and takes step proportional (learning rate) to the negative of the gradient.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The gradient is calculated by taking the partial derivative for every parameter in the loss function: (take linear regression as an example)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$l = \frac{1}{N}\sum_{i=1}^{N}(y_i-(wx_i+b))^2$$&lt;/p&gt;
&lt;p&gt;$$\frac{\partial l}{\partial w} = \frac{1}{N}\sum_{i=1}^{N}-2x_i(y_i-(wx_i+b))$$&lt;/p&gt;
&lt;p&gt;$$\frac{\partial l}{\partial b} = \frac{1}{N}\sum_{i=1}^{N}-2(y_i-(wx_i+b))$$&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;An epoch consists of using the training set entirely to update each parameter. At each epoch, we update $w$ and $b$ ($\alpha$ is the learning rate):&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$w \leftarrow w - \alpha\frac{\partial l}{\partial w}$$&lt;/p&gt;
&lt;p&gt;$$b \leftarrow b - \alpha\frac{\partial l}{\partial b}$$&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Minibatch stochastic gradient descent is a version that speed up the computation by approximating the gradient using smaller batches.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;particularities&#34;&gt;Particularities&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Some algorithms, like decision tree learning, can accept categorical features while others expect numerical values. All algorithms in scikit-learn expect numerical features.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some algorithms, like SVM, allow weightings for each class. If the weighting is higher, the algorithm tries not to make errors in predicting training examples of this class.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some classification models, like SVM and kNN, only output the class. Others, like logistic regression or decision trees, can also return the score between 0 and 1 (can be used as a confidence score).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some classification algorithms - like decision trees, logistic regression, or SVM - build the model using the whole dataset at once. Others can be trained iteratively, one batch at a time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some algorithms, like decision trees/SVM/kNN, can be used for both classification and regression, while others can only solve one type of problem.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.goodreads.com/book/show/43190851-the-hundred-page-machine-learning-book&#34;&gt;The Hundred-Page Machine Learning Book&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
