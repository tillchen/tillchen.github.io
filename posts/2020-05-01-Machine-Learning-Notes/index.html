<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Tianyao Chen">
    <meta name="description" content="Some notes for basic machine learning concepts.">
    <meta name="keywords" content="Tianyao Chen, Till Chen, tech, software, AI, ML">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Machine Learning Notes"/>
<meta name="twitter:description" content="Some notes for basic machine learning concepts."/>

    <meta property="og:title" content="Machine Learning Notes" />
<meta property="og:description" content="Some notes for basic machine learning concepts." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tillchen.com/posts/2020-05-01-Machine-Learning-Notes/" />
<meta property="article:published_time" content="2020-05-01T09:25:01+02:00" />
<meta property="article:modified_time" content="2020-05-01T09:25:01+02:00" />


    
      <base href="https://tillchen.com/posts/2020-05-01-Machine-Learning-Notes/">
    
    <title>
  Machine Learning Notes · Tianyao Chen
</title>

    
      <link rel="canonical" href="https://tillchen.com/posts/2020-05-01-Machine-Learning-Notes/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css" integrity="sha384-Bfad6CLCknfcloXFOyFnlgtENryhrpZCe29RTifKEixXQZ38WheV+i/6YWSzkz3V" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="https://tillchen.com/css/coder.min.a4f332213a21ce8eb521670c614470c58923aaaf385e2a73982c31dd7642decb.css" integrity="sha256-pPMyITohzo61IWcMYURwxYkjqq84XipzmCwx3XZC3ss=" crossorigin="anonymous" media="screen" />
    

    

    
      
        
        
        <link rel="stylesheet" href="https://tillchen.com/css/coder-dark.min.e78e80fc3a585a4d1c8fc7f58623b6ff852411e38431a9cd1792877ecaa160f6.css" integrity="sha256-546A/DpYWk0cj8f1hiO2/4UkEeOEManNF5KHfsqhYPY=" crossorigin="anonymous" media="screen" />
      
    

    

    

    <link rel="icon" type="image/png" href="https://tillchen.com/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://tillchen.com/images/favicon-16x16.png" sizes="16x16">

    <meta name="generator" content="Hugo 0.69.2" />
  </head>

  
  
    
  
  <body class="colorscheme-auto">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="https://tillchen.com/">
      Tianyao Chen
    </a>
    
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://tillchen.com/about/">About</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://tillchen.com/posts/">Blog</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://tillchen.com/tags/">Tags</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://tillchen.com/projects/">Projects</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://tillchen.com/skills/">Skills</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://tillchen.com/testimonials/">Testimonials</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://tillchen.com/pdfs/Tianyao_Chen_resume.pdf">Resume</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://tillchen.com/contact/">Contact</a>
          </li>
        
      
      
    </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Machine Learning Notes</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fas fa-calendar"></i>
              <time datetime='2020-05-01T09:25:01&#43;02:00'>
                May 1, 2020
              </time>
            </span>
            <span class="reading-time">
              <i class="fas fa-clock"></i>
              11-minute read
            </span>
          </div>
          
          <div class="tags">
  <i class="fas fa-tag"></i>
    <a href="https://tillchen.com/tags/Machine-Learning/">Machine Learning</a>
      <span class="separator">•</span>
    <a href="https://tillchen.com/tags/AI/">AI</a></div>

        </div>
      </header>

      <div>
        
        <ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#types-of-learning">Types of Learning</a>
<ul>
<li><a href="#supervised-learning">Supervised learning</a></li>
<li><a href="#unsupervised-learning">Unsupervised learning</a></li>
<li><a href="#semi-supervised-learning">Semi-supervised learning</a></li>
<li><a href="#reinforcement-learning">Reinforcement learning</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#notations-and-definitions">Notations and Definitions</a></li>
<li><a href="#fundamental-algorithms">Fundamental Algorithms</a>
<ul>
<li><a href="#linear-regression">Linear Regression</a></li>
<li><a href="#logistic-regression">Logistic Regression</a></li>
<li><a href="#decision-tree-learning">Decision Tree Learning</a></li>
<li><a href="#support-vector-machine">Support Vector Machine</a>
<ul>
<li><a href="#dealing-with-noise">Dealing With Noise</a>
<ul>
<li><a href="#dealing-with-inherent-non-linearity-kernels">Dealing with Inherent Non-Linearity (Kernels)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#k-nearest-neighbors-knn">k-Nearest Neighbors (kNN)</a></li>
</ul>
</li>
<li><a href="#anatomy-of-a-learning-algorithm">Anatomy of a Learning Algorithm</a>
<ul>
<li><a href="#building-blocks">Building blocks</a></li>
<li><a href="#gradient-descent">Gradient Descent</a></li>
<li><a href="#particularities">Particularities</a></li>
</ul>
</li>
<li><a href="#basic-practice">Basic Practice</a>
<ul>
<li><a href="#feature-engineering">Feature Engineering</a>
<ul>
<li><a href="#one-hot-encoding-categorical---numerical">One-Hot Encoding (Categorical -&gt; Numerical)</a></li>
<li><a href="#binning-bucketing-numerical---categorical">Binning (Bucketing) (Numerical -&gt; Categorical)</a></li>
<li><a href="#normalization">Normalization</a></li>
<li><a href="#standardization-z-score-normalization">Standardization (Z-score Normalization)</a></li>
<li><a href="#dealing-with-missing-features">Dealing with Missing Features</a></li>
</ul>
</li>
<li><a href="#learning-algorithm-selection">Learning Algorithm Selection</a></li>
<li><a href="#three-sets">Three Sets</a></li>
<li><a href="#underfitting-and-overfitting">Underfitting and Overfitting</a></li>
<li><a href="#regularization">Regularization</a></li>
<li><a href="#model-performance-assessment">Model Performance Assessment</a></li>
<li><a href="#hyperparameter-tuning">Hyperparameter Tuning</a>
<ul>
<li><a href="#cross-validation">Cross-Validation</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#neural-networks-and-deep-learning">Neural Networks and Deep Learning</a>
<ul>
<li><a href="#neural-networks">Neural Networks</a>
<ul>
<li><a href="#multilayer-perceptron-vanilla-neural-network">Multilayer Perceptron (Vanilla Neural Network)</a></li>
<li><a href="#feed-forward-neural-network-architecture">Feed-Forward Neural Network Architecture</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>This post is about some basic machine learning concepts.</p>
<h3 id="types-of-learning">Types of Learning</h3>
<h4 id="supervised-learning">Supervised learning</h4>
<ul>
<li>In a feature vector $x_i$ (i=1&hellip;N), a single feature is denoted by $x^j$ (j=1&hellip;D). And $y_i$ is the label.</li>
<li>The goal is to produce a model that takes a feature vector x as input and outputs the label.</li>
</ul>
<h4 id="unsupervised-learning">Unsupervised learning</h4>
<ul>
<li>The goal is either to transform the input feature vector into another vector (dimensionality reduction) or into a value (clustering, outlier detection).</li>
</ul>
<h4 id="semi-supervised-learning">Semi-supervised learning</h4>
<ul>
<li>The dataset has both labeled and unlabeled (much higher quantity) data.</li>
<li>The same goal as supervised learning.</li>
<li>The hope is that the unlabeled data can help produce a better model. This is because a larger sample reflects better the probability distribution of the source of the labeled data.</li>
</ul>
<h4 id="reinforcement-learning">Reinforcement learning</h4>
<ul>
<li>It perceives the <strong>state</strong> of the environment as a feature vector and then execute <strong>actions</strong> in every state, which bring <strong>rewards</strong> and move the machine to another state.</li>
<li>The goal is to learn a <strong>policy</strong>: a function that takes the feature vector as input and outputs an optimal action that maximizes the expected average reward.</li>
<li>It&rsquo;s suited for problems whose decision making is sequential and the goal is long-term.</li>
</ul>
<h2 id="notations-and-definitions">Notations and Definitions</h2>
<ol>
<li>
<p>$x_{l,u}^{(j)}$ in a neural net means the feature j of unit u in layer l.</p>
</li>
<li>
<p>$x$ is by default a column vector and $x^T$ a row vector. Then vector is on the left side of a matrix, we usually do $x^TA$.</p>
</li>
<li>
<p>$arg max_{a \in A}f(a)$ returns the element a of the set A that maximizes f(a).</p>
</li>
<li>
<p>$\mathbb{E}[\hat\theta(S_X) = \theta]$, where $\hat\theta(S_X)$ is the unbiased estimator of some statistic $\theta$ (e.g. $\mu$) calculated using a sample $S_X$.</p>
</li>
<li>
<p>A hyperparameter is a parameter whose value is set before the learning process begins. By contrast, the values of other parameters are derived via training.</p>
</li>
<li>
<p>Model-based learning vs instance-based learning:</p>
<ul>
<li>Model-based: creates a model with parameters learned from the data. (Most supervised learning algorithms)</li>
<li>Instance-based: uses the whole dataset as the model. Instead of performing explicit generalization, it compares new problem instances with instances seen in training (e.g. kNN).</li>
</ul>
</li>
<li>
<p>Shallow vs deep learning:</p>
<ul>
<li>Shallow learning: learns the parameters of the model directly from the features of the training examples.</li>
<li>Deep (neural network) learning: uses neural nets with more than one layer and learns the parameters from the outputs of the preceding layers instead.</li>
</ul>
</li>
</ol>
<h2 id="fundamental-algorithms">Fundamental Algorithms</h2>
<h3 id="linear-regression">Linear Regression</h3>
<ol>
<li>We want a model as a linear combination of features of example x ($w$ is a D-dimensional vector of parameters and b is a real number.):</li>
</ol>
<p>$$f_{w,b}(x) = wx+b$$</p>
<ol start="2">
<li>We find the optimal values $w^{*}$ and $b^{*}$ by minimizing the cost function below (the empirical risk - the average loss):</li>
</ol>
<p>$$\frac{1}{N}\sum_{i=1&hellip;N}(f_{w,b}(x_i)-y_i)^2$$</p>
<ol start="3">
<li>
<p>Overfitting: a model that predicts well for the training data but not for data unseen during training. Linear regression rarely overfits.</p>
</li>
<li>
<p>The squared loss is used since it&rsquo;s convinent. The absolute loss function doesn&rsquo;t have a continuous derivative, which makes it not smooth. Unsmooth functions create difficulties for optimization problems in linear algebra.</p>
</li>
<li>
<p>To find the extrema (minima or maxima), we can simply set the gradient to zero.</p>
</li>
</ol>
<h3 id="logistic-regression">Logistic Regression</h3>
<ol>
<li>
<p>Logistic regression is not a regression, but a classification learning algorithm. The names is because the mathematical formulation is similar to linear regression.</p>
</li>
<li>
<p>The standard logistic function or the sigmoid function:</p>
</li>
</ol>
<p>$$f(x)=\frac{1}{1+e^{-x}}$$</p>
<p><img src="https://tillchen.com/images/logistic_function.png" alt="Logistic Function"></p>
<ol start="3">
<li>The logistic regression model:</li>
</ol>
<p>$$f(x)=\frac{1}{1+e^{-(wx+b)}}$$</p>
<ol start="4">
<li>
<p>For binary classification, we can say the label is positive if the probability is higher than or equal to 0.5.</p>
</li>
<li>
<p>We optimize by using the maximum likelihood (the goodness of fit) ($y_i$ is either 0 or 1):</p>
</li>
</ol>
<p>$$L_{w,b}=\prod_{i=1&hellip;N}f_{w,b}(x_i)^{y_i}(1-f_{w,b}(x_i))^{1-y_i}$$</p>
<ol start="6">
<li>In practice, it&rsquo;s more convenient to maximize the log-likelihood (since ln is a strictly increasing function, maximizing this is the same as maximizing its argument):</li>
</ol>
<p>$$LogL_{w,b}=ln(L_{w,b})=\sum_{i=1}^Ny_iln(f_{w,b}(x_i)) + (1-y_i)ln(1-f_{w,b}(x_i))$$</p>
<ol start="7">
<li>Unlike linear regression, there&rsquo;s no closed form solution. A typical numerical optimization procedure is gradient descent.</li>
</ol>
<h3 id="decision-tree-learning">Decision Tree Learning</h3>
<p>To be filled.</p>
<h3 id="support-vector-machine">Support Vector Machine</h3>
<p>To be filled.</p>
<h4 id="dealing-with-noise">Dealing With Noise</h4>
<p>To be filled.</p>
<h5 id="dealing-with-inherent-non-linearity-kernels">Dealing with Inherent Non-Linearity (Kernels)</h5>
<ol>
<li>
<p>The kernel trick: use a function to <em>implicitly</em> transform the original space into a higher dimensional space during the cost function optimization. (We hope that data will be linearly separable in the transformed space).</p>
</li>
<li>
<p>We use kernel functions or simply kernels to efficiently work in higher-dimensional spaces without doing this transformation explicitly.</p>
</li>
<li>
<p>By using the kernel trick, we get rid of a costly transformation to the higher dimension and avoid computing their dot-product.</p>
</li>
<li>
<p>We can use the quadratic kernel $k(x_i, x_k) = (x_ix_k)^2$ here. Another widely used kernel is the RBF (radial basis function) kernel: $k(x, x^{'}) = exp(-\frac{||x-x^{'}||^2}{2\sigma^2})$. It has infinite dimensions. We can vary the hyperparameter $\sigma$  and choose wether to get a smooth of curvy decision boundary.</p>
</li>
</ol>
<h3 id="k-nearest-neighbors-knn">k-Nearest Neighbors (kNN)</h3>
<ol>
<li>
<p>kNN is a non-parametric learning algorithm. Contrary to other learning algorithms that allow discarding the training data after the model is built, kNN keeps all training examples in memory.</p>
</li>
<li>
<p>The closeness of two examples is given by a distance function. Other than the Euclidean distance, we could also use the cosine similarity function ($0^{\circ}: 1; 90^{\circ}: 0; 180^{\circ}: -1$).</p>
</li>
</ol>
<h2 id="anatomy-of-a-learning-algorithm">Anatomy of a Learning Algorithm</h2>
<h3 id="building-blocks">Building blocks</h3>
<ol>
<li>A loss function</li>
<li>An optimization criterion based on the loss function (e.g. a cost function)</li>
<li>An optimization routine leveraging training data to find a solution to the optimization criterion.</li>
</ol>
<h3 id="gradient-descent">Gradient Descent</h3>
<ol>
<li>
<p>To find a local minimum, start at some random point and takes step proportional (learning rate) to the negative of the gradient.</p>
</li>
<li>
<p>The gradient is calculated by taking the partial derivative for every parameter in the loss function: (take linear regression as an example)</p>
</li>
</ol>
<p>$$l = \frac{1}{N}\sum_{i=1}^{N}(y_i-(wx_i+b))^2$$</p>
<p>$$\frac{\partial l}{\partial w} = \frac{1}{N}\sum_{i=1}^{N}-2x_i(y_i-(wx_i+b))$$</p>
<p>$$\frac{\partial l}{\partial b} = \frac{1}{N}\sum_{i=1}^{N}-2(y_i-(wx_i+b))$$</p>
<ol start="3">
<li>An epoch consists of using the training set entirely to update each parameter. At each epoch, we update $w$ and $b$ ($\alpha$ is the learning rate):</li>
</ol>
<p>$$w \leftarrow w - \alpha\frac{\partial l}{\partial w}$$</p>
<p>$$b \leftarrow b - \alpha\frac{\partial l}{\partial b}$$</p>
<ol start="4">
<li>Minibatch stochastic gradient descent is a version that speed up the computation by approximating the gradient using smaller batches.</li>
</ol>
<h3 id="particularities">Particularities</h3>
<ol>
<li>
<p>Some algorithms, like decision tree learning, can accept categorical features while others expect numerical values. All algorithms in scikit-learn expect numerical features.</p>
</li>
<li>
<p>Some algorithms, like SVM, allow weightings for each class. If the weighting is higher, the algorithm tries not to make errors in predicting training examples of this class.</p>
</li>
<li>
<p>Some classification models, like SVM and kNN, only output the class. Others, like logistic regression or decision trees, can also return the score between 0 and 1 (can be used as a confidence score).</p>
</li>
<li>
<p>Some classification algorithms - like decision trees, logistic regression, or SVM - build the model using the whole dataset at once. Others can be trained iteratively, one batch at a time.</p>
</li>
<li>
<p>Some algorithms, like decision trees/SVM/kNN, can be used for both classification and regression, while others can only solve one type of problem.</p>
</li>
</ol>
<h2 id="basic-practice">Basic Practice</h2>
<h3 id="feature-engineering">Feature Engineering</h3>
<ol>
<li>
<p>Feature engineering: the problem of transforming raw data into a dataset.</p>
</li>
<li>
<p>The role of the data analyst is to create informative features with high predictive power.</p>
</li>
<li>
<p>We say a model has a low bias when it predicts the training data well.</p>
</li>
</ol>
<h4 id="one-hot-encoding-categorical---numerical">One-Hot Encoding (Categorical -&gt; Numerical)</h4>
<ol>
<li>
<p>One-hot is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0). It transforms a categorical feature into several binary ones.</p>
</li>
<li>
<p>Example: red = [1,0,0]; yellow = [0,1,0]; green = [0,0,1].</p>
</li>
<li>
<p>When the ordering matters, we can just do: {poor, decent, good, excellent} -&gt; {1, 2, 3, 4}.</p>
</li>
</ol>
<h4 id="binning-bucketing-numerical---categorical">Binning (Bucketing) (Numerical -&gt; Categorical)</h4>
<ol>
<li>Binning converts a continuous feature in to multiple binary features called bins, typically based on value range.</li>
</ol>
<h4 id="normalization">Normalization</h4>
<ol>
<li>Normalization converts a numerical value into a value typically in the interval [-1,1] or [0,1], which increases the speed of learning:</li>
</ol>
<p>$$\bar x^{(j)} = \frac{x^{(j)}-min^{(j)}}{max^{(j)}-min^{(j)}}$$</p>
<h4 id="standardization-z-score-normalization">Standardization (Z-score Normalization)</h4>
<ol>
<li>
<p>Standardization rescales the feature values so that they have the properties of a standard normal distribution with $\mu = 0$ and  $\sigma = 1$.</p>
</li>
<li>
<p>Standard scores or z-scores can be calculated as follows:</p>
</li>
</ol>
<p>$$\hat x^{(j)} = \frac{x^{(j)} - \mu^{(j)}}{\sigma^{(j)}}$$</p>
<ol start="3">
<li>Normalization vs Standardization:
<ul>
<li>Unsupervised learning - standardization.</li>
<li>The distribution is close to a normal distribution - standardization.</li>
<li>Have outliers - standardization (normalization will squeeze the normal values into a very small range).</li>
<li>Other cases - normalization.</li>
</ul>
</li>
</ol>
<h4 id="dealing-with-missing-features">Dealing with Missing Features</h4>
<ol>
<li>
<p>Remove the examples with missing features</p>
</li>
<li>
<p>Use a learning algorithm that can deal with missing feature values.</p>
</li>
<li>
<p>Use Data imputation (the process of replacing missing data with substituted values)</p>
<ul>
<li>Replace the missing value with an average value.</li>
<li>Replace the missing value with a value outside the normal range.</li>
<li>Replace the missing value with a value in the middle of the range.</li>
<li>Use the missing value as the target for a regression problem.</li>
<li>Increase the dimensionality by adding a binary indicator feature.</li>
</ul>
</li>
</ol>
<h3 id="learning-algorithm-selection">Learning Algorithm Selection</h3>
<ul>
<li>Explainability</li>
<li>In-memory vs. out-of-memory (batch or incremental/online)</li>
<li>Number of features and examples (neural networks for a huge number of features)</li>
<li>Categorical vs. numerical features</li>
<li>Nonlinearity of the data (SVM/linear regression/linear kernel/logistic regression of lineraly separable data)</li>
<li>Training speed (neural networks are slow to train)</li>
<li>Prediction speed</li>
</ul>
<p><img src="https://tillchen.com/images/ml_map.png" alt="scikit-learn cheat sheet"></p>
<h3 id="three-sets">Three Sets</h3>
<ol>
<li>
<p>Three sets:</p>
<ul>
<li>Training set (95%)</li>
<li>Validation set (2.5%): choose the learning algorithm and find the best values of hyperparameters.</li>
<li>Test set (2.5%): assess the model before delivery and production.</li>
</ul>
</li>
<li>
<p>The last two are also called holdout sets, which are not used to build the model.</p>
</li>
</ol>
<h3 id="underfitting-and-overfitting">Underfitting and Overfitting</h3>
<ol>
<li>
<p>Underfitting: the model makes many mistakes on the training data - high bias.</p>
<ul>
<li>The model is too simple.</li>
<li>The features are not informative enough.</li>
</ul>
</li>
<li>
<p>Overfitting: the model predicts very well the training data but poorly the data from the holdout sets. - high variance (the error due to the sensitivity to small fluctuations in the training set)</p>
<ul>
<li>The model is too complex.</li>
<li>Too many features but a small number of training examples.</li>
</ul>
</li>
<li>
<p>The overfitting model learns the idiosyncrasies of the training set:</p>
<ul>
<li>the noise</li>
<li>the sampling imperfection due to the small dataset size</li>
<li>other artifacts extrinsic to the decision problem</li>
</ul>
</li>
<li>
<p>Solutions for overfitting:</p>
<ul>
<li>Try a simply model</li>
<li>Reduce the dimensionality</li>
<li>Add more training data</li>
<li>Regularization - the most widely used approach</li>
</ul>
</li>
</ol>
<h3 id="regularization">Regularization</h3>
<ol>
<li>
<p>Regularization forces the learning algorithm to build a less complex model, which leads to a slightly higher bias but a much lower variance - the bias-variance tradeoff.</p>
</li>
<li>
<p>The two most widely used are L1 and L2 regularization, which add a penalizing term whose value is higher when the model is more complex.</p>
</li>
<li>
<p>For linear regression, L1 looks like this ($|\mathbf{w}| = \sum_{i=1}^D|w^{(j)}|$ and C is a hyperparameter), which tries to set most $w^{(j)}$ to value small values or zero (|| means abs here):</p>
</li>
</ol>
<p>$$\min_{\mathbf{w},b} \left[C|\mathbf{w}| + \frac{1}{N}\sum_{i=1&hellip;N}(f_{\mathbf{w},b}(\mathbf{x}_i)-y_i)^2\right]$$</p>
<ol start="4">
<li>For linear regression, L2 looks like this (($||\mathbf{w}||^2 = \sum_{i=1}^D(w^{(j)})^2$):</li>
</ol>
<p>$$\min_{\mathbf{w},b} \left[C||\mathbf{w}||^2 + \frac{1}{N}\sum_{i=1&hellip;N}(f_{\mathbf{w},b}(\mathbf{x}_i)-y_i)^2\right]$$</p>
<ol start="5">
<li>
<p>L1 produces a sparse model with most parameters equal to zero if C is large enough, which makes feature selection that decides which features are essential and increases explainability. L2 gives better results if our only goal is to maximize the performance on holdout sets. Plus L2 is differentiable - graident descent.</p>
</li>
<li>
<p>L1 and L2 are the special cases for elastic net regularization. L1 - lasso; L2 - ridge regularization.</p>
</li>
</ol>
<h3 id="model-performance-assessment">Model Performance Assessment</h3>
<p>To be filled.</p>
<h3 id="hyperparameter-tuning">Hyperparameter Tuning</h3>
<p>To be filled.</p>
<h4 id="cross-validation">Cross-Validation</h4>
<ol>
<li>
<p>When we have few training examples, it could be prohibitive to have both validation and test set. So we can split the data into a training and a test set and use cross-validation on the training set to simulate a validation set.</p>
</li>
<li>
<p>Each subset of the training set is called a fold. Typically, a five-fold cross-validation is used in practice. We would train five models (use $F_2 F_3 F_4 F_5$ for model 1 and so on) and compute the value of the metric of interest on each validation set, from $F_1$ to $F_5$ and get the average score.</p>
</li>
</ol>
<h2 id="neural-networks-and-deep-learning">Neural Networks and Deep Learning</h2>
<h3 id="neural-networks">Neural Networks</h3>
<ol>
<li>The function $f_{NN}$ is a nested function. So for a 3-layer neural network that returns a scalar:</li>
</ol>
<p>$$y=f_{NN}(\mathbf{x}) = f_3(\mathbf{f}_2(\mathbf{f}_1(\mathbf{x})))$$</p>
<ol>
<li>$\mathbf{f}_1$ and $\mathbf{f}_2$ are vector functions of the following form ($\mathbf{g}_l$ is the activation function - a fixed and chosen nonlinear function; $\mathbf{W}_l$ (a matrix) and $\mathbf{b}_l$ (a vector) are learned using gradient descent):</li>
</ol>
<p>$$\mathbf{f}_l(\mathbf{z}) = \mathbf{g}_l(\mathbf{W}_l\mathbf{z} + \mathbf{b}_l)$$</p>
<h4 id="multilayer-perceptron-vanilla-neural-network">Multilayer Perceptron (Vanilla Neural Network)</h4>
<ol>
<li>
<p>A multiplayer perceptron (MLP) is a class of a feed-forward neural network (FFNN). It has a fully-connected architecture.</p>
</li>
<li>
<p>In each rectangle unit:</p>
<ol>
<li>All inputs of the unit are joined together to form an input vector.</li>
<li>The unit applies a linear transformation.</li>
<li>The unit applies an activation function.</li>
</ol>
</li>
<li>
<p>An example (each unit&rsquo;s parameters $\mathbf{w}<em>{l,u}$ and $b</em>{l,u}$):</p>
</li>
</ol>
<p><img src="https://tillchen.com/images/mlp.png" alt="MLP"></p>
<h4 id="feed-forward-neural-network-architecture">Feed-Forward Neural Network Architecture</h4>
<ol>
<li>
<p>If we want to solve a regression or a classification problem, the last layer usually contains only one unit. If $g_{last}$ is linear, then the neural net is a regression model. If $g_{last}$ is a logistic function, it&rsquo;s a binary classification model.</p>
</li>
<li>
<p>Any differentiable function can be chosen as $g_{l,u}$. And primary reason of such a nonlinear component is to allow the neural net to approximate nonlinear functions.</p>
</li>
<li>
<p>Popular choices for $g$ are the logistic function, TanH, and ReLU (rectified linear unit function):</p>
</li>
</ol>
<p>$$tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$</p>
<p>$$relu(z) = 0 \ \ if \ z &lt; 0$$</p>
<p>$$relu(z)= z \ \ otherwise$$</p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://www.goodreads.com/book/show/43190851-the-hundred-page-machine-learning-book">The Hundred-Page Machine Learning Book</a></li>
</ul>

      </div>


      <footer>
        


        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "tillchen" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
        
        
      </footer>
    </article>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js" id="MathJax-script"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [
          ['$', '$'], ['\\(', '\\)']
        ],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
        <p>Sharing thoughts on Computer Science, Software Engineering, AI, Machine Learning, and Data Science.</p>
      
      
        ©
        
        2020
         Tianyao Chen 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
    </section>
  </footer>

    </main>

    

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-139593886-1', 'auto');
	
	ga('send', 'pageview');
}
</script>


    

  </body>

</html>
