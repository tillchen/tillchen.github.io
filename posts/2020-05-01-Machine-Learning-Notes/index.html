<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Tianyao Chen">
    <meta name="description" content="Some notes for basic machine learning concepts.">
    <meta name="keywords" content="Tianyao Chen, Till Chen, tech, software, AI, ML">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Machine Learning Notes"/>
<meta name="twitter:description" content="Some notes for basic machine learning concepts."/>

    <meta property="og:title" content="Machine Learning Notes" />
<meta property="og:description" content="Some notes for basic machine learning concepts." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tillchen.com/posts/2020-05-01-Machine-Learning-Notes/" />
<meta property="article:published_time" content="2020-05-01T09:25:01+02:00" />
<meta property="article:modified_time" content="2020-05-01T09:25:01+02:00" />


    
      <base href="https://tillchen.com/posts/2020-05-01-Machine-Learning-Notes/">
    
    <title>
  Machine Learning Notes · Tianyao Chen
</title>

    
      <link rel="canonical" href="https://tillchen.com/posts/2020-05-01-Machine-Learning-Notes/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css" integrity="sha384-Bfad6CLCknfcloXFOyFnlgtENryhrpZCe29RTifKEixXQZ38WheV+i/6YWSzkz3V" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="https://tillchen.com/css/coder.min.a4f332213a21ce8eb521670c614470c58923aaaf385e2a73982c31dd7642decb.css" integrity="sha256-pPMyITohzo61IWcMYURwxYkjqq84XipzmCwx3XZC3ss=" crossorigin="anonymous" media="screen" />
    

    

    
      
        
        
        <link rel="stylesheet" href="https://tillchen.com/css/coder-dark.min.e78e80fc3a585a4d1c8fc7f58623b6ff852411e38431a9cd1792877ecaa160f6.css" integrity="sha256-546A/DpYWk0cj8f1hiO2/4UkEeOEManNF5KHfsqhYPY=" crossorigin="anonymous" media="screen" />
      
    

    

    

    <link rel="icon" type="image/png" href="https://tillchen.com/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://tillchen.com/images/favicon-16x16.png" sizes="16x16">

    <meta name="generator" content="Hugo 0.69.2" />
  </head>

  
  
    
  
  <body class="colorscheme-auto">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="https://tillchen.com/">
      Tianyao Chen
    </a>
    
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://tillchen.com/about/">About</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://tillchen.com/posts/">Blog</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://tillchen.com/tags/">Tags</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://tillchen.com/projects/">Projects</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://tillchen.com/skills/">Skills</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://tillchen.com/testimonials/">Testimonials</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://tillchen.com/pdfs/Tianyao_Chen_resume.pdf">Resume</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://tillchen.com/contact/">Contact</a>
          </li>
        
      
      
    </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Machine Learning Notes</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fas fa-calendar"></i>
              <time datetime='2020-05-01T09:25:01&#43;02:00'>
                May 1, 2020
              </time>
            </span>
            <span class="reading-time">
              <i class="fas fa-clock"></i>
              6-minute read
            </span>
          </div>
          
          <div class="tags">
  <i class="fas fa-tag"></i>
    <a href="https://tillchen.com/tags/Machine-Learning/">Machine Learning</a>
      <span class="separator">•</span>
    <a href="https://tillchen.com/tags/AI/">AI</a></div>

        </div>
      </header>

      <div>
        
        <ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#types-of-learning">Types of Learning</a>
<ul>
<li><a href="#supervised-learning">Supervised learning</a></li>
<li><a href="#unsupervised-learning">Unsupervised learning</a></li>
<li><a href="#semi-supervised-learning">Semi-supervised learning</a></li>
<li><a href="#reinforcement-learning">Reinforcement learning</a></li>
</ul>
</li>
<li><a href="#notations-and-definitions">Notations and Definitions</a></li>
</ul>
</li>
<li><a href="#fundamental-algorithms">Fundamental Algorithms</a>
<ul>
<li><a href="#linear-regression">Linear Regression</a></li>
<li><a href="#logistic-regression">Logistic Regression</a></li>
<li><a href="#decision-tree-learning">Decision Tree Learning</a></li>
<li><a href="#support-vector-machine">Support Vector Machine</a>
<ul>
<li><a href="#dealing-with-noise">Dealing With Noise</a>
<ul>
<li><a href="#dealing-with-inherent-non-linearity-kernels">Dealing with Inherent Non-Linearity (Kernels)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#k-nearest-neighbors-knn">k-Nearest Neighbors (kNN)</a></li>
</ul>
</li>
<li><a href="#anatomy-of-a-learning-algorithm">Anatomy of a Learning Algorithm</a>
<ul>
<li><a href="#building-blocks">Building blocks</a></li>
<li><a href="#gradient-descent">Gradient Descent</a></li>
<li><a href="#particularities">Particularities</a></li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>This post is about some basic machine learning concepts.</p>
<h3 id="types-of-learning">Types of Learning</h3>
<h4 id="supervised-learning">Supervised learning</h4>
<ul>
<li>In a feature vector $x_i$ (i=1&hellip;N), a single feature is denoted by $x^j$ (j=1&hellip;D). And $y_i$ is the label.</li>
<li>The goal is to produce a model that takes a feature vector x as input and outputs the label.</li>
</ul>
<h4 id="unsupervised-learning">Unsupervised learning</h4>
<ul>
<li>The goal is either to transform the input feature vector into another vector (dimensionality reduction) or into a value (clustering, outlier detection).</li>
</ul>
<h4 id="semi-supervised-learning">Semi-supervised learning</h4>
<ul>
<li>The dataset has both labeled and unlabeled (much higher quantity) data.</li>
<li>The same goal as supervised learning.</li>
<li>The hope is that the unlabeled data can help produce a better model. This is because a larger sample reflects better the probability distribution of the source of the labeled data.</li>
</ul>
<h4 id="reinforcement-learning">Reinforcement learning</h4>
<ul>
<li>It perceives the <strong>state</strong> of the environment as a feature vector and then execute <strong>actions</strong> in every state, which bring <strong>rewards</strong> and move the machine to another state.</li>
<li>The goal is to learn a <strong>policy</strong>: a function that takes the feature vector as input and outputs an optimal action that maximizes the expected average reward.</li>
<li>It&rsquo;s suited for problems whose decision making is sequential and the goal is long-term.</li>
</ul>
<h3 id="notations-and-definitions">Notations and Definitions</h3>
<ol>
<li>
<p>$x_{l,u}^{(j)}$ in a neural net means the feature j of unit u in layer l.</p>
</li>
<li>
<p>$x$ is by default a column vector and $x^T$ a row vector. Then vector is on the left side of a matrix, we usually do $x^TA$.</p>
</li>
<li>
<p>$arg max_{a \in A}f(a)$ returns the element a of the set A that maximizes f(a).</p>
</li>
<li>
<p>$\mathbb{E}[\hat\theta(S_X) = \theta]$, where $\hat\theta(S_X)$ is the unbiased estimator of some statistic $\theta$ (e.g. $\mu$) calculated using a sample $S_X$.</p>
</li>
<li>
<p>A hyperparameter is a parameter whose value is set before the learning process begins. By contrast, the values of other parameters are derived via training.</p>
</li>
<li>
<p>Model-based learning vs instance-based learning:</p>
<ul>
<li>Model-based: creates a model with parameters learned from the data. (Most supervised learning algorithms)</li>
<li>Instance-based: uses the whole dataset as the model. Instead of performing explicit generalization, it compares new problem instances with instances seen in training (e.g. kNN).</li>
</ul>
</li>
<li>
<p>Shallow vs deep learning:</p>
<ul>
<li>Shallow learning: learns the parameters of the model directly from the features of the training examples.</li>
<li>Deep (neural network) learning: uses neural nets with more than one layer and learns the parameters from the outputs of the preceding layers instead.</li>
</ul>
</li>
</ol>
<h2 id="fundamental-algorithms">Fundamental Algorithms</h2>
<h3 id="linear-regression">Linear Regression</h3>
<ol>
<li>We want a model as a linear combination of features of example x ($w$ is a D-dimensional vector of parameters and b is a real number.):</li>
</ol>
<p>$$f_{w,b}(x) = wx+b$$</p>
<ol start="2">
<li>We find the optimal values $w^{*}$ and $b^{*}$ by minimizing the cost function below (the empirical risk - the average loss):</li>
</ol>
<p>$$\frac{1}{N}\sum_{i=1&hellip;N}(f_{w,b}(x_i)-y_i)^2$$</p>
<ol start="3">
<li>
<p>Overfitting: a model that predicts well for the training data but not for data unseen during training. Linear regression rarely overfits.</p>
</li>
<li>
<p>The squared loss is used since it&rsquo;s convinent. The absolute loss function doesn&rsquo;t have a continuous derivative, which makes it not smooth. Unsmooth functions create difficulties for optimization problems in linear algebra.</p>
</li>
<li>
<p>To find the extrema (minima or maxima), we can simply set the gradient to zero.</p>
</li>
</ol>
<h3 id="logistic-regression">Logistic Regression</h3>
<ol>
<li>
<p>Logistic regression is not a regression, but a classification learning algorithm. The names is because the mathematical formulation is similar to linear regression.</p>
</li>
<li>
<p>The standard logistic function or the sigmoid function:</p>
</li>
</ol>
<p>$$f(x)=\frac{1}{1+e^{-x}}$$</p>
<p><img src="https://tillchen.com/images/logistic_function.png" alt="Logistic Function"></p>
<ol start="3">
<li>The logistic regression model:</li>
</ol>
<p>$$f(x)=\frac{1}{1+e^{-(wx+b)}}$$</p>
<ol start="4">
<li>
<p>For binary classification, we can say the label is positive if the probability is higher than or equal to 0.5.</p>
</li>
<li>
<p>We optimize by using the maximum likelihood (the goodness of fit) ($y_i$ is either 0 or 1):</p>
</li>
</ol>
<p>$$L_{w,b}=\prod_{i=1&hellip;N}f_{w,b}(x_i)^{y_i}(1-f_{w,b}(x_i))^{1-y_i}$$</p>
<ol start="6">
<li>In practice, it&rsquo;s more convenient to maximize the log-likelihood (since ln is a strictly increasing function, maximizing this is the same as maximizing its argument):</li>
</ol>
<p>$$LogL_{w,b}=ln(L_{w,b})=\sum_{i=1}^Ny_iln(f_{w,b}(x_i)) + (1-y_i)ln(1-f_{w,b}(x_i))$$</p>
<ol start="7">
<li>Unlike linear regression, there&rsquo;s no closed form solution. A typical numerical optimization procedure is gradient descent.</li>
</ol>
<h3 id="decision-tree-learning">Decision Tree Learning</h3>
<p>To be filled.</p>
<h3 id="support-vector-machine">Support Vector Machine</h3>
<p>To be filled.</p>
<h4 id="dealing-with-noise">Dealing With Noise</h4>
<p>To be filled.</p>
<h5 id="dealing-with-inherent-non-linearity-kernels">Dealing with Inherent Non-Linearity (Kernels)</h5>
<ol>
<li>
<p>The kernel trick: use a function to <em>implicitly</em> transform the original space into a higher dimensional space during the cost function optimization. (We hope that data will be linearly separable in the transformed space).</p>
</li>
<li>
<p>We use kernel functions or simply kernels to efficiently work in higher-dimensional spaces without doing this transformation explicitly.</p>
</li>
<li>
<p>By using the kernel trick, we get rid of a costly transformation to the higher dimension and avoid computing their dot-product.</p>
</li>
<li>
<p>We can use the quadratic kernel $k(x_i, x_k) = (x_ix_k)^2$ here. Another widely used kernel is the RBF (radial basis function) kernel: $k(x, x^{'}) = exp(-\frac{||x-x^{'}||^2}{2\sigma^2})$. It has infinite dimensions. We can vary the hyperparameter $\sigma$  and choose wether to get a smooth of curvy decision boundary.</p>
</li>
</ol>
<h3 id="k-nearest-neighbors-knn">k-Nearest Neighbors (kNN)</h3>
<ol>
<li>
<p>kNN is a non-parametric learning algorithm. Contrary to other learning algorithms that allow discarding the training data after the model is built, kNN keeps all training examples in memory.</p>
</li>
<li>
<p>The closeness of two examples is given by a distance function. Other than the Euclidean distance, we could also use the cosine similarity function ($0^{\circ}: 1; 90^{\circ}: 0; 180^{\circ}: -1$).</p>
</li>
</ol>
<h2 id="anatomy-of-a-learning-algorithm">Anatomy of a Learning Algorithm</h2>
<h3 id="building-blocks">Building blocks</h3>
<ol>
<li>A loss function</li>
<li>An optimization criterion based on the loss function (e.g. a cost function)</li>
<li>An optimization routine leveraging training data to find a solution to the optimization criterion.</li>
</ol>
<h3 id="gradient-descent">Gradient Descent</h3>
<ol>
<li>
<p>To find a local minimum, start at some random point and takes step proportional (learning rate) to the negative of the gradient.</p>
</li>
<li>
<p>The gradient is calculated by taking the partial derivative for every parameter in the loss function: (take linear regression as an example)</p>
</li>
</ol>
<p>$$l = \frac{1}{N}\sum_{i=1}^{N}(y_i-(wx_i+b))^2$$</p>
<p>$$\frac{\partial l}{\partial w} = \frac{1}{N}\sum_{i=1}^{N}-2x_i(y_i-(wx_i+b))$$</p>
<p>$$\frac{\partial l}{\partial b} = \frac{1}{N}\sum_{i=1}^{N}-2(y_i-(wx_i+b))$$</p>
<ol start="3">
<li>An epoch consists of using the training set entirely to update each parameter. At each epoch, we update $w$ and $b$ ($\alpha$ is the learning rate):</li>
</ol>
<p>$$w \leftarrow w - \alpha\frac{\partial l}{\partial w}$$</p>
<p>$$b \leftarrow b - \alpha\frac{\partial l}{\partial b}$$</p>
<ol start="4">
<li>Minibatch stochastic gradient descent is a version that speed up the computation by approximating the gradient using smaller batches.</li>
</ol>
<h3 id="particularities">Particularities</h3>
<ol>
<li>
<p>Some algorithms, like decision tree learning, can accept categorical features while others expect numerical values. All algorithms in scikit-learn expect numerical features.</p>
</li>
<li>
<p>Some algorithms, like SVM, allow weightings for each class. If the weighting is higher, the algorithm tries not to make errors in predicting training examples of this class.</p>
</li>
<li>
<p>Some classification models, like SVM and kNN, only output the class. Others, like logistic regression or decision trees, can also return the score between 0 and 1 (can be used as a confidence score).</p>
</li>
<li>
<p>Some classification algorithms - like decision trees, logistic regression, or SVM - build the model using the whole dataset at once. Others can be trained iteratively, one batch at a time.</p>
</li>
<li>
<p>Some algorithms, like decision trees/SVM/kNN, can be used for both classification and regression, while others can only solve one type of problem.</p>
</li>
</ol>
<h2 id="references">References</h2>
<ul>
<li><a href="https://www.goodreads.com/book/show/43190851-the-hundred-page-machine-learning-book">The Hundred-Page Machine Learning Book</a></li>
</ul>

      </div>


      <footer>
        


        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "tillchen" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
        
        
      </footer>
    </article>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js" id="MathJax-script"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [
          ['$', '$'], ['\\(', '\\)']
        ],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
        <p>Sharing thoughts on Computer Science, Software Engineering, AI, Machine Learning, and Data Science.</p>
      
      
        ©
        
        2020
         Tianyao Chen 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
    </section>
  </footer>

    </main>

    

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-139593886-1', 'auto');
	
	ga('send', 'pageview');
}
</script>


    

  </body>

</html>
