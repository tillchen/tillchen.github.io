<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>Machine Learning Notes - Tianyao Chen</title><meta name="Description" content="Some notes for basic machine learning concepts."><meta property="og:title" content="Machine Learning Notes" />
<meta property="og:description" content="Some notes for basic machine learning concepts." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tillchen.com/posts/2020-05-01-Machine-Learning-Notes/" />
<meta property="og:image" content="https://tillchen.com/images/Tianyao_Chen_resume.jpeg"/>
<meta property="article:published_time" content="2020-05-01T09:25:01+02:00" />
<meta property="article:modified_time" content="2020-05-01T09:25:01+02:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://tillchen.com/images/Tianyao_Chen_resume.jpeg"/>

<meta name="twitter:title" content="Machine Learning Notes"/>
<meta name="twitter:description" content="Some notes for basic machine learning concepts."/>
<meta name="application-name" content="Tianyao Chen">
<meta name="apple-mobile-web-app-title" content="Tianyao Chen"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://tillchen.com/posts/2020-05-01-Machine-Learning-Notes/" /><link rel="prev" href="https://tillchen.com/posts/2020-4-10-Algorithms-and-Data-Structures/" /><link rel="next" href="https://tillchen.com/posts/2020-05-21-Software-Engineering-Notes/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.055364f5be272caa092b0e6654c165828707f8ab971e2656383a6d6392bc345e.css" integrity="sha256-BVNk9b4nLKoJKw5mVMFlgocH&#43;KuXHiZWODptY5K8NF4="><link rel="stylesheet" href="/css/style.min.65cc6a26e228ec18570a0247a015df4f84fcec2c12fe2804894af3e7c7477591.css" integrity="sha256-ZcxqJuIo7BhXCgJHoBXfT4T87CwS/igEiUrz58dHdZE="><link rel="stylesheet" href="/lib/fontawesome-free/all.min.876d023d9d10c97941b80c3b03e2a5b94631ff7a4af9cee5604a6a2d39718d84.css" integrity="sha256-h20CPZ0QyXlBuAw7A&#43;KluUYx/3pK&#43;c7lYEpqLTlxjYQ="><link rel="stylesheet" href="/lib/animate/animate.min.3c770e90f98eb21b0c042fafb49755af93306fbaf42e449524f94fae9fc83295.css" integrity="sha256-PHcOkPmOshsMBC&#43;vtJdVr5Mwb7r0LkSVJPlPrp/IMpU="><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Machine Learning Notes",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/tillchen.com\/posts\/2020-05-01-Machine-Learning-Notes\/"
        },"image": {
                "@type": "ImageObject",
                "url": "https:\/\/tillchen.com\/images\/resume_photo.jpeg",
                "width":  800 ,
                "height":  600 
            },"genre": "posts","keywords": "Machine Learning, AI","wordcount":  4565 ,
        "url": "https:\/\/tillchen.com\/posts\/2020-05-01-Machine-Learning-Notes\/","datePublished": "2020-05-01T09:25:01+02:00","dateModified": "2020-05-01T09:25:01+02:00","publisher": {
                "@type": "Organization",
                "name": "Tianyao Chen",
                "logo": {
                "@type": "ImageObject",
                "url": "https:\/\/tillchen.com\/images\/resume_photo.jpeg",
                "width":  127 ,
                "height":  40 
                }
            },"author": {
                "@type": "Person",
                "name": "Tianyao Chen"
            },"description": "Some notes for basic machine learning concepts."
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Tianyao Chen">Tianyao Chen</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/about/"> About </a><a class="menu-item" href="/posts/"> Blog </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/projects/"> Projects </a><a class="menu-item" href="/skills/"> Skills </a><a class="menu-item" href="/testimonials/"> Testimonials </a><a class="menu-item" href="/pdfs/Tianyao_Chen_resume.pdf"> Resume </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Tianyao Chen">Tianyao Chen</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/about/" title="">About</a><a class="menu-item" href="/posts/" title="">Blog</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/projects/" title="">Projects</a><a class="menu-item" href="/skills/" title="">Skills</a><a class="menu-item" href="/testimonials/" title="">Testimonials</a><a class="menu-item" href="/pdfs/Tianyao_Chen_resume.pdf" title="">Resume</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">Machine Learning Notes</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://tillchen.com" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw"></i>Tianyao Chen</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="May 1, 2020">May 1, 2020</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;4565 words&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;22 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a>
      <ul>
        <li><a href="#types-of-learning">Types of Learning</a>
          <ul>
            <li><a href="#supervised-learning">Supervised learning</a></li>
            <li><a href="#unsupervised-learning">Unsupervised learning</a></li>
            <li><a href="#semi-supervised-learning">Semi-supervised learning</a></li>
            <li><a href="#reinforcement-learning">Reinforcement learning</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#notations-and-definitions">Notations and Definitions</a>
      <ul>
        <li><a href="#statistical-decision-theory">Statistical Decision Theory</a></li>
      </ul>
    </li>
    <li><a href="#fundamental-algorithms">Fundamental Algorithms</a>
      <ul>
        <li><a href="#linear-regression">Linear Regression</a></li>
        <li><a href="#logistic-regression">Logistic Regression</a></li>
        <li><a href="#decision-tree-learning">Decision Tree Learning</a></li>
        <li><a href="#support-vector-machine">Support Vector Machine</a>
          <ul>
            <li><a href="#dealing-with-noise">Dealing With Noise</a>
              <ul>
                <li><a href="#dealing-with-inherent-non-linearity-kernels">Dealing with Inherent Non-Linearity (Kernels)</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#k-nearest-neighbors-knn">k-Nearest Neighbors (kNN)</a></li>
      </ul>
    </li>
    <li><a href="#anatomy-of-a-learning-algorithm">Anatomy of a Learning Algorithm</a>
      <ul>
        <li><a href="#building-blocks">Building blocks</a></li>
        <li><a href="#gradient-descent">Gradient Descent</a></li>
        <li><a href="#particularities">Particularities</a></li>
      </ul>
    </li>
    <li><a href="#basic-practice">Basic Practice</a>
      <ul>
        <li><a href="#feature-engineering">Feature Engineering</a>
          <ul>
            <li><a href="#one-hot-encoding-categorical---numerical">One-Hot Encoding (Categorical -&gt; Numerical)</a></li>
            <li><a href="#binning-bucketing-numerical---categorical">Binning (Bucketing) (Numerical -&gt; Categorical)</a></li>
            <li><a href="#normalization">Normalization</a></li>
            <li><a href="#standardization-z-score-normalization">Standardization (Z-score Normalization)</a></li>
            <li><a href="#dealing-with-missing-features">Dealing with Missing Features</a></li>
          </ul>
        </li>
        <li><a href="#learning-algorithm-selection">Learning Algorithm Selection</a></li>
        <li><a href="#three-sets">Three Sets</a></li>
        <li><a href="#underfitting-and-overfitting">Underfitting and Overfitting</a></li>
        <li><a href="#regularization">Regularization</a>
          <ul>
            <li><a href="#shrinkage-ridge-regression">Shrinkage: Ridge Regression</a></li>
          </ul>
        </li>
        <li><a href="#model-performance-assessment">Model Performance Assessment</a>
          <ul>
            <li><a href="#confusion-matrix">Confusion Matrix</a></li>
            <li><a href="#precisionrecall">Precision/Recall</a></li>
            <li><a href="#accuracy">Accuracy</a></li>
            <li><a href="#cost-sensitive-accuracy">Cost-Sensitive Accuracy</a></li>
            <li><a href="#area-under-the-roc-curve-auc">Area under the ROC Curve (AUC)</a></li>
          </ul>
        </li>
        <li><a href="#hyperparameter-tuning">Hyperparameter Tuning</a>
          <ul>
            <li><a href="#training-error">Training Error</a></li>
            <li><a href="#generalizationtest-error">Generalization/Test Error</a></li>
            <li><a href="#validation-set-error-estimation">Validation Set Error Estimation</a></li>
            <li><a href="#cross-validation">Cross-Validation</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#neural-networks-and-deep-learning">Neural Networks and Deep Learning</a>
      <ul>
        <li><a href="#neural-networks">Neural Networks</a>
          <ul>
            <li><a href="#multilayer-perceptron-vanilla-neural-network">Multilayer Perceptron (Vanilla Neural Network)</a></li>
            <li><a href="#feed-forward-neural-network-architecture">Feed-Forward Neural Network Architecture</a></li>
          </ul>
        </li>
        <li><a href="#deep-learning">Deep Learning</a>
          <ul>
            <li><a href="#convolutional-neural-network">Convolutional Neural Network</a></li>
            <li><a href="#recurrent-neural-network">Recurrent Neural Network</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#problems-and-solutions">Problems and Solutions</a>
      <ul>
        <li><a href="#basis-expansions">Basis Expansions</a>
          <ul>
            <li><a href="#regression-with-gaussian-kernel">Regression with Gaussian Kernel</a></li>
          </ul>
        </li>
        <li><a href="#kernel-regression">Kernel Regression</a></li>
        <li><a href="#classification">Classification</a>
          <ul>
            <li><a href="#linear-discriminant-analysis">Linear Discriminant Analysis</a></li>
          </ul>
        </li>
        <li><a href="#ensemble-learning">Ensemble Learning</a>
          <ul>
            <li><a href="#boosting-and-bagging">Boosting and Bagging</a></li>
            <li><a href="#random-forest">Random Forest</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#unsupervised-learning-1">Unsupervised Learning</a>
      <ul>
        <li><a href="#density-estimation">Density Estimation</a></li>
        <li><a href="#clustering">Clustering</a>
          <ul>
            <li><a href="#k-means">K-Means</a></li>
          </ul>
        </li>
        <li><a href="#dimensionality-reduction">Dimensionality Reduction</a>
          <ul>
            <li><a href="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#references">References</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><ul>
<li><a href="#introduction" rel="">Introduction</a>
<ul>
<li><a href="#types-of-learning" rel="">Types of Learning</a>
<ul>
<li><a href="#supervised-learning" rel="">Supervised learning</a></li>
<li><a href="#unsupervised-learning" rel="">Unsupervised learning</a></li>
<li><a href="#semi-supervised-learning" rel="">Semi-supervised learning</a></li>
<li><a href="#reinforcement-learning" rel="">Reinforcement learning</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#notations-and-definitions" rel="">Notations and Definitions</a>
<ul>
<li><a href="#statistical-decision-theory" rel="">Statistical Decision Theory</a></li>
</ul>
</li>
<li><a href="#fundamental-algorithms" rel="">Fundamental Algorithms</a>
<ul>
<li><a href="#linear-regression" rel="">Linear Regression</a></li>
<li><a href="#logistic-regression" rel="">Logistic Regression</a></li>
<li><a href="#decision-tree-learning" rel="">Decision Tree Learning</a></li>
<li><a href="#support-vector-machine" rel="">Support Vector Machine</a>
<ul>
<li><a href="#dealing-with-noise" rel="">Dealing With Noise</a>
<ul>
<li><a href="#dealing-with-inherent-non-linearity-kernels" rel="">Dealing with Inherent Non-Linearity (Kernels)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#k-nearest-neighbors-knn" rel="">k-Nearest Neighbors (kNN)</a></li>
</ul>
</li>
<li><a href="#anatomy-of-a-learning-algorithm" rel="">Anatomy of a Learning Algorithm</a>
<ul>
<li><a href="#building-blocks" rel="">Building blocks</a></li>
<li><a href="#gradient-descent" rel="">Gradient Descent</a></li>
<li><a href="#particularities" rel="">Particularities</a></li>
</ul>
</li>
<li><a href="#basic-practice" rel="">Basic Practice</a>
<ul>
<li><a href="#feature-engineering" rel="">Feature Engineering</a>
<ul>
<li><a href="#one-hot-encoding-categorical---numerical" rel="">One-Hot Encoding (Categorical -&gt; Numerical)</a></li>
<li><a href="#binning-bucketing-numerical---categorical" rel="">Binning (Bucketing) (Numerical -&gt; Categorical)</a></li>
<li><a href="#normalization" rel="">Normalization</a></li>
<li><a href="#standardization-z-score-normalization" rel="">Standardization (Z-score Normalization)</a></li>
<li><a href="#dealing-with-missing-features" rel="">Dealing with Missing Features</a></li>
</ul>
</li>
<li><a href="#learning-algorithm-selection" rel="">Learning Algorithm Selection</a></li>
<li><a href="#three-sets" rel="">Three Sets</a></li>
<li><a href="#underfitting-and-overfitting" rel="">Underfitting and Overfitting</a></li>
<li><a href="#regularization" rel="">Regularization</a>
<ul>
<li><a href="#shrinkage-ridge-regression" rel="">Shrinkage: Ridge Regression</a></li>
</ul>
</li>
<li><a href="#model-performance-assessment" rel="">Model Performance Assessment</a>
<ul>
<li><a href="#confusion-matrix" rel="">Confusion Matrix</a></li>
<li><a href="#precisionrecall" rel="">Precision/Recall</a></li>
<li><a href="#accuracy" rel="">Accuracy</a></li>
<li><a href="#cost-sensitive-accuracy" rel="">Cost-Sensitive Accuracy</a></li>
<li><a href="#area-under-the-roc-curve-auc" rel="">Area under the ROC Curve (AUC)</a></li>
</ul>
</li>
<li><a href="#hyperparameter-tuning" rel="">Hyperparameter Tuning</a>
<ul>
<li><a href="#training-error" rel="">Training Error</a></li>
<li><a href="#generalizationtest-error" rel="">Generalization/Test Error</a></li>
<li><a href="#validation-set-error-estimation" rel="">Validation Set Error Estimation</a></li>
<li><a href="#cross-validation" rel="">Cross-Validation</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#neural-networks-and-deep-learning" rel="">Neural Networks and Deep Learning</a>
<ul>
<li><a href="#neural-networks" rel="">Neural Networks</a>
<ul>
<li><a href="#multilayer-perceptron-vanilla-neural-network" rel="">Multilayer Perceptron (Vanilla Neural Network)</a></li>
<li><a href="#feed-forward-neural-network-architecture" rel="">Feed-Forward Neural Network Architecture</a></li>
</ul>
</li>
<li><a href="#deep-learning" rel="">Deep Learning</a>
<ul>
<li><a href="#convolutional-neural-network" rel="">Convolutional Neural Network</a></li>
<li><a href="#recurrent-neural-network" rel="">Recurrent Neural Network</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#problems-and-solutions" rel="">Problems and Solutions</a>
<ul>
<li><a href="#basis-expansions" rel="">Basis Expansions</a>
<ul>
<li><a href="#regression-with-gaussian-kernel" rel="">Regression with Gaussian Kernel</a></li>
</ul>
</li>
<li><a href="#kernel-regression" rel="">Kernel Regression</a></li>
<li><a href="#classification" rel="">Classification</a>
<ul>
<li><a href="#linear-discriminant-analysis" rel="">Linear Discriminant Analysis</a></li>
</ul>
</li>
<li><a href="#ensemble-learning" rel="">Ensemble Learning</a>
<ul>
<li><a href="#boosting-and-bagging" rel="">Boosting and Bagging</a></li>
<li><a href="#random-forest" rel="">Random Forest</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#unsupervised-learning-1" rel="">Unsupervised Learning</a>
<ul>
<li><a href="#density-estimation" rel="">Density Estimation</a></li>
<li><a href="#clustering" rel="">Clustering</a>
<ul>
<li><a href="#k-means" rel="">K-Means</a></li>
</ul>
</li>
<li><a href="#dimensionality-reduction" rel="">Dimensionality Reduction</a>
<ul>
<li><a href="#principal-component-analysis-pca" rel="">Principal Component Analysis (PCA)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#references" rel="">References</a></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>This post is about some basic machine learning concepts.</p>
<h3 id="types-of-learning">Types of Learning</h3>
<h4 id="supervised-learning">Supervised learning</h4>
<ul>
<li>In a feature vector $x_i$ (i=1&hellip;N), a single feature is denoted by $x^j$ (j=1&hellip;D). And $y_i$ is the label.</li>
<li>The goal is to produce a model that takes a feature vector x as input and outputs the label.</li>
</ul>
<h4 id="unsupervised-learning">Unsupervised learning</h4>
<ul>
<li>The goal is either to transform the input feature vector into another vector (dimensionality reduction) or into a value (clustering, outlier detection).</li>
</ul>
<h4 id="semi-supervised-learning">Semi-supervised learning</h4>
<ul>
<li>The dataset has both labeled and unlabeled (much higher quantity) data.</li>
<li>The same goal as supervised learning.</li>
<li>The hope is that the unlabeled data can help produce a better model. This is because a larger sample reflects better the probability distribution of the source of the labeled data.</li>
</ul>
<h4 id="reinforcement-learning">Reinforcement learning</h4>
<ul>
<li>It perceives the <strong>state</strong> of the environment as a feature vector and then execute <strong>actions</strong> in every state, which bring <strong>rewards</strong> and move the machine to another state.</li>
<li>The goal is to learn a <strong>policy</strong>: a function that takes the feature vector as input and outputs an optimal action that maximizes the expected average reward.</li>
<li>It&rsquo;s suited for problems whose decision making is sequential and the goal is long-term.</li>
</ul>
<h2 id="notations-and-definitions">Notations and Definitions</h2>
<ol>
<li>
<p>$x_{l,u}^{(j)}$ in a neural net means the feature j of unit u in layer l.</p>
</li>
<li>
<p>$x$ is by default a column vector and $x^T$ a row vector. Then vector is on the left side of a matrix, we usually do $x^TA$.</p>
</li>
<li>
<p>$argmax_{a \in A}f(a)$ returns the element a of the set A that maximizes f(a).</p>
</li>
<li>
<p>$\mathbb{E}[\hat\theta(S_X) = \theta]$, where $\hat\theta(S_X)$ is the unbiased estimator of some statistic $\theta$ (e.g. $\mu$) calculated using a sample $S_X$.</p>
</li>
<li>
<p>A hyperparameter is a parameter whose value is set before the learning process begins. By contrast, the values of other parameters are derived via training.</p>
</li>
<li>
<p>Model-based learning vs instance-based learning:</p>
<ul>
<li>Model-based: creates a model with parameters learned from the data. (Most supervised learning algorithms)</li>
<li>Instance-based: uses the whole dataset as the model. Instead of performing explicit generalization, it compares new problem instances with instances seen in training (e.g. kNN).</li>
</ul>
</li>
<li>
<p>Shallow vs deep learning:</p>
<ul>
<li>Shallow learning: learns the parameters of the model directly from the features of the training examples.</li>
<li>Deep (neural network) learning: uses neural nets with more than one layer and learns the parameters from the outputs of the preceding layers instead.</li>
</ul>
</li>
</ol>
<h3 id="statistical-decision-theory">Statistical Decision Theory</h3>
<ol>
<li>
<p>Quantitative outputs are called $Y$ with $Y = (Y_1,&hellip;,Y_K)^T$; and qualitative outputs are called $G$ with $G = (G_1,&hellip;,G_K)^T$ (group).</p>
</li>
<li>
<p>Loss functions:</p>
<p>$$L_2(y, y&rsquo;) = (y-y&rsquo;)^2$$</p>
<p>$$L_{0-1}(g, g&rsquo;) = 0 \ (g=g&rsquo;) \ or \ 1 \ (g\neq g&rsquo;)$$</p>
<p>$$L_1(y,y&rsquo;) = |y-y'|$$</p>
</li>
<li>
<p>Expected prediction error (EPE):</p>
<p>$$EPE(f)=E(L_2(Y,f(X)))$$</p>
<p>$$EPE(f)=E(L_{0-1}(G,f(X)))$$</p>
</li>
<li>
<p>The function f that minimizes EPE is give by: ($E(Y|X=x)$ is called a regressor/regression function)</p>
<p>$$f(x)=E(Y|X=x)$$</p>
</li>
<li>
<p>Bayes classifier:</p>
<p>$$f(x)=argmin_{g \in R_G}[1-p(g|x)] \Rightarrow$$</p>
<p>$$f(x)=\mathbf{g} \ if \ p(\mathbf{g}|x) = \max_{g \in R_G}p(g|x)$$</p>
</li>
<li>
<p>Expected squared prediction error of $\hat f$ at location $x_0$:</p>
<p>$$EPE(\hat f, x_0)=E(L_2(Y,f(X))|X=x_0)$$</p>
</li>
<li>
<p>Error = Irreducible Error + $\text{Bias}^2$ + Variance</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/error_decomposition.png"
        data-srcset="/images/error_decomposition.png, /images/error_decomposition.png 1.5x, /images/error_decomposition.png 2x"
        data-sizes="auto"
        alt="error decomposition"
        title="error decomposition" /></p>
</li>
</ol>
<h2 id="fundamental-algorithms">Fundamental Algorithms</h2>
<h3 id="linear-regression">Linear Regression</h3>
<ol>
<li>
<p>We want a model as a linear combination of features of example x ($w$ is a D-dimensional vector of parameters and b is a real number.):</p>
<p>$$f_{w,b}(x) = wx+b$$</p>
</li>
<li>
<p>We find the optimal values $w^{*}$ and $b^{*}$ by minimizing the cost function below (the empirical risk - the average loss):</p>
<p>$$\frac{1}{N}\sum_{i=1&hellip;N}(f_{w,b}(x_i)-y_i)^2$$</p>
</li>
<li>
<p>Overfitting: a model that predicts well for the training data but not for data unseen during training. Linear regression rarely overfits.</p>
</li>
<li>
<p>The squared loss is used since it&rsquo;s convinent. The absolute loss function doesn&rsquo;t have a continuous derivative, which makes it not smooth. Unsmooth functions create difficulties for optimization problems in linear algebra.</p>
</li>
<li>
<p>To find the extrema (minima or maxima), we can simply set the gradient to zero.</p>
</li>
<li>
<p>The linear model is an approximation for the regressor $E(Y|X)$ of the form: ($\beta_0$ is the intercept or bias)</p>
<p>$$f_{\beta}(X) = \beta_0+\sum_{j=1}^DX_j\beta_j$$</p>
</li>
<li>
<p>By using $\beta = (\beta_0,&hellip;,\beta_D)^T$ and $Z=(1,X_1,&hellip;,X_D)^T$, we can rewrite the model to:</p>
<p>$$f_{\beta}(Z)=Z^T\beta$$</p>
</li>
<li>
<p>The least squares estimator (parameter estimator $\gamma$):</p>
<p>$$\gamma = argmin_{\gamma}\sum_{i=1}^N(y_i-f_{\gamma}(x_i))^2$$</p>
</li>
<li>
<p>Solution for the estimator:</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/linear_regression_least_squares.png"
        data-srcset="/images/linear_regression_least_squares.png, /images/linear_regression_least_squares.png 1.5x, /images/linear_regression_least_squares.png 2x"
        data-sizes="auto"
        alt="Linear Regression by Least Squares"
        title="Linear Regression by Least Squares" /></p>
</li>
<li>
<p>The algorithm:</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/linear_regression.png"
        data-srcset="/images/linear_regression.png, /images/linear_regression.png 1.5x, /images/linear_regression.png 2x"
        data-sizes="auto"
        alt="Linear Regression"
        title="Linear Regression" /></p>
</li>
<li>
<p>Find local minimum/maximum:</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/min_max.png"
        data-srcset="/images/min_max.png, /images/min_max.png 1.5x, /images/min_max.png 2x"
        data-sizes="auto"
        alt="min max"
        title="min max" /></p>
<p>$$M \ \text{positive definite} \Leftrightarrow x^TMx&gt;0 \ \text{for all }x \in \mathbb{R}^n \backslash \mathbf{0}$$</p>
<p>$$M \ \text{negative definite} \Leftrightarrow x^TMx&lt;0 \ \text{for all }x \in \mathbb{R}^n \backslash \mathbf{0}$$</p>
</li>
<li>
<p>Statistical analysis with the additive noise:</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/analysis_for_linear_regression.png"
        data-srcset="/images/analysis_for_linear_regression.png, /images/analysis_for_linear_regression.png 1.5x, /images/analysis_for_linear_regression.png 2x"
        data-sizes="auto"
        alt="statistical analysis"
        title="statistical analysis" /></p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/variance.png"
        data-srcset="/images/variance.png, /images/variance.png 1.5x, /images/variance.png 2x"
        data-sizes="auto"
        alt="variance"
        title="variance" /></p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/z_score.png"
        data-srcset="/images/z_score.png, /images/z_score.png 1.5x, /images/z_score.png 2x"
        data-sizes="auto"
        alt="z-score"
        title="z-score" /></p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/z_score_rule_of_thumb.png"
        data-srcset="/images/z_score_rule_of_thumb.png, /images/z_score_rule_of_thumb.png 1.5x, /images/z_score_rule_of_thumb.png 2x"
        data-sizes="auto"
        alt="z-score rule of thumb"
        title="z-score rule of thumb" /></p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/z_score_example.png"
        data-srcset="/images/z_score_example.png, /images/z_score_example.png 1.5x, /images/z_score_example.png 2x"
        data-sizes="auto"
        alt="z-score example"
        title="z-score example" /></p>
</li>
<li>
<p>For a K-dimensional output:</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/linear_regression_k_d_output.png"
        data-srcset="/images/linear_regression_k_d_output.png, /images/linear_regression_k_d_output.png 1.5x, /images/linear_regression_k_d_output.png 2x"
        data-sizes="auto"
        alt="linear regression K-D output"
        title="linear regression K-D output" /></p>
</li>
</ol>
<h3 id="logistic-regression">Logistic Regression</h3>
<ol>
<li>
<p>Logistic regression is not a regression, but a classification learning algorithm. The names is because the mathematical formulation is similar to linear regression.</p>
</li>
<li>
<p>The standard logistic function or the sigmoid function:</p>
<p>$$f(x)=\frac{1}{1+e^{-x}}$$</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/logistic_function.png"
        data-srcset="/images/logistic_function.png, /images/logistic_function.png 1.5x, /images/logistic_function.png 2x"
        data-sizes="auto"
        alt="Logistic Function"
        title="Logistic Function" /></p>
</li>
<li>
<p>The logistic regression model:</p>
<p>$$f(x)=\frac{1}{1+e^{-(wx+b)}}$$</p>
</li>
<li>
<p>For binary classification, we can say the label is positive if the probability is higher than or equal to 0.5.</p>
</li>
<li>
<p>We optimize by using the maximum likelihood (the goodness of fit) ($y_i$ is either 0 or 1):</p>
<p>$$L_{w,b}=\prod_{i=1&hellip;N}f_{w,b}(x_i)^{y_i}(1-f_{w,b}(x_i))^{1-y_i}$$</p>
</li>
<li>
<p>In practice, it&rsquo;s more convenient to maximize the log-likelihood (since ln is a strictly increasing function, maximizing this is the same as maximizing its argument):</p>
<p>$$LogL_{w,b}=ln(L_{w,b})=\sum_{i=1}^Ny_iln(f_{w,b}(x_i)) + (1-y_i)ln(1-f_{w,b}(x_i))$$</p>
</li>
<li>
<p>Unlike linear regression, there&rsquo;s no closed form solution. A typical numerical optimization procedure is gradient descent.</p>
</li>
</ol>
<h3 id="decision-tree-learning">Decision Tree Learning</h3>
<ol>
<li>
<p>If the value is below a specific threshold, the left branch is followed. Otherwise, the right branch is followed. The decision is made about the class when the leaf node is reached.</p>
</li>
<li>
<p>We consider ID3 (Iterative Dichotomiser 3), which also optimizes the average log-likelihood like logistic regression. It does some approximately by constructing a nonparametric model $f_{ID3} = Pr(y=1|\mathbf{x})$.</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/decision_tree_learning.png"
        data-srcset="/images/decision_tree_learning.png, /images/decision_tree_learning.png 1.5x, /images/decision_tree_learning.png 2x"
        data-sizes="auto"
        alt="decision tree learning"
        title="decision tree learning" /></p>
</li>
<li>
<p>Entropy is a measure of uncertainty about a random variable. We find a split that minimizes the entropy.</p>
</li>
</ol>
<h3 id="support-vector-machine">Support Vector Machine</h3>
<ol>
<li>
<p>The model $f(\mathbf{x}) = sign(\mathbf{w^{*}x} - b^{*})$ (sign returns +1 if it&rsquo;s positive and -1 if it&rsquo;s negative) (The sign * means the optimal value.)</p>
</li>
<li>
<p>We prefer the hyperplane with the largest margin, which is the distance between the closest examples of two classes. A large margin contributes to a better generalization. We achieve this by minimizing the norm of $\mathbf{w}$: $||\mathbf{w}||$.</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/SVM.png"
        data-srcset="/images/SVM.png, /images/SVM.png 1.5x, /images/SVM.png 2x"
        data-sizes="auto"
        alt="SVM"
        title="SVM" /></p>
</li>
</ol>
<h4 id="dealing-with-noise">Dealing With Noise</h4>
<ol>
<li>We can use the hinge loss function: $\max (0, 1 - y_i(\mathbf{wx}_i - b))$ for data that&rsquo;s not linearly separable.</li>
</ol>
<h5 id="dealing-with-inherent-non-linearity-kernels">Dealing with Inherent Non-Linearity (Kernels)</h5>
<ol>
<li>
<p>The kernel trick: use a function to <em>implicitly</em> transform the original space into a higher dimensional space during the cost function optimization. (We hope that data will be linearly separable in the transformed space).</p>
</li>
<li>
<p>We use kernel functions or simply kernels to efficiently work in higher-dimensional spaces without doing this transformation explicitly.</p>
</li>
<li>
<p>By using the kernel trick, we get rid of a costly transformation to the higher dimension and avoid computing their dot-product.</p>
</li>
<li>
<p>We can use the quadratic kernel $k(x_i, x_k) = (x_ix_k)^2$ here. Another widely used kernel is the RBF (radial basis function) kernel: $k(x, x^{'}) = exp(-\frac{||x-x^{'}||^2}{2\sigma^2})$. It has infinite dimensions. We can vary the hyperparameter $\sigma$  and choose wether to get a smooth of curvy decision boundary.</p>
</li>
</ol>
<h3 id="k-nearest-neighbors-knn">k-Nearest Neighbors (kNN)</h3>
<ol>
<li>
<p>kNN is a non-parametric learning algorithm. Contrary to other learning algorithms that allow discarding the training data after the model is built, kNN keeps all training examples in memory.</p>
</li>
<li>
<p>The closeness of two examples is given by a distance function. Other than the Euclidean distance, we could also use the cosine similarity function ($0^{\circ}: 1; 90^{\circ}: 0; 180^{\circ}: -1$).</p>
</li>
<li>
<p>Algorithms</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/kNN_regression.png"
        data-srcset="/images/kNN_regression.png, /images/kNN_regression.png 1.5x, /images/kNN_regression.png 2x"
        data-sizes="auto"
        alt="kNN regression"
        title="kNN regression" /></p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/kNN_classification.png"
        data-srcset="/images/kNN_classification.png, /images/kNN_classification.png 1.5x, /images/kNN_classification.png 2x"
        data-sizes="auto"
        alt="kNN classification"
        title="kNN classification" /></p>
</li>
<li>
<p>The bias-variance decomposition:</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/kNN_bias_variance_decomposition.png"
        data-srcset="/images/kNN_bias_variance_decomposition.png, /images/kNN_bias_variance_decomposition.png 1.5x, /images/kNN_bias_variance_decomposition.png 2x"
        data-sizes="auto"
        alt="kNN bias-variance decomposition"
        title="kNN bias-variance decomposition" /></p>
</li>
<li>
<p>The bias-variance trade-off:</p>
<ul>
<li>k=1: if $x_0$ is in the training data, the bias = 0; the largest variance. (similar to overfitting)</li>
<li>k=N: the largest bias (the predictor is constant); the smallest variance. (similar to underfitting)</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/kNN_error_example.png"
        data-srcset="/images/kNN_error_example.png, /images/kNN_error_example.png 1.5x, /images/kNN_error_example.png 2x"
        data-sizes="auto"
        alt="kNN error example"
        title="kNN error example" /></p>
</li>
</ol>
<h2 id="anatomy-of-a-learning-algorithm">Anatomy of a Learning Algorithm</h2>
<h3 id="building-blocks">Building blocks</h3>
<ol>
<li>A loss function</li>
<li>An optimization criterion based on the loss function (e.g. a cost function)</li>
<li>An optimization routine leveraging training data to find a solution to the optimization criterion.</li>
</ol>
<h3 id="gradient-descent">Gradient Descent</h3>
<ol>
<li>
<p>To find a local minimum, start at some random point and takes step proportional (learning rate) to the negative of the gradient.</p>
</li>
<li>
<p>The gradient is calculated by taking the partial derivative for every parameter in the loss function: (take linear regression as an example)</p>
<p>$$l = \frac{1}{N}\sum_{i=1}^{N}(y_i-(wx_i+b))^2$$</p>
<p>$$\frac{\partial l}{\partial w} = \frac{1}{N}\sum_{i=1}^{N}-2x_i(y_i-(wx_i+b))$$</p>
<p>$$\frac{\partial l}{\partial b} = \frac{1}{N}\sum_{i=1}^{N}-2(y_i-(wx_i+b))$$</p>
</li>
<li>
<p>An epoch consists of using the training set entirely to update each parameter. (One sweep through full training data.) At each epoch, we update $w$ and $b$ ($\alpha$ is the learning rate):</p>
<p>$$w \leftarrow w - \alpha\frac{\partial l}{\partial w}$$</p>
<p>$$b \leftarrow b - \alpha\frac{\partial l}{\partial b}$$</p>
</li>
<li>
<p>Minibatch stochastic gradient descent is a version that speed up the computation by approximating the gradient using smaller batches.</p>
</li>
<li>
<p>Issues with batch GD:</p>
<ul>
<li>Might get stuck in local minimum.</li>
<li>One update, very expensive for large N.</li>
</ul>
</li>
<li>
<p>Stochastic gradient descent: use only one training sample to do update. (In each step, pick randomly and update)</p>
<ul>
<li>Each step is cheap.</li>
<li>Can easily escape local minima.</li>
<li>Many more steps necessary.</li>
<li>Jumping around the solution.</li>
</ul>
</li>
</ol>
<h3 id="particularities">Particularities</h3>
<ol>
<li>
<p>Some algorithms, like decision tree learning, can accept categorical features while others expect numerical values. All algorithms in scikit-learn expect numerical features.</p>
</li>
<li>
<p>Some algorithms, like SVM, allow weightings for each class. If the weighting is higher, the algorithm tries not to make errors in predicting training examples of this class.</p>
</li>
<li>
<p>Some classification models, like SVM and kNN, only output the class. Others, like logistic regression or decision trees, can also return the score between 0 and 1 (can be used as a confidence score).</p>
</li>
<li>
<p>Some classification algorithms - like decision trees, logistic regression, or SVM - build the model using the whole dataset at once. Others can be trained iteratively, one batch at a time.</p>
</li>
<li>
<p>Some algorithms, like decision trees/SVM/kNN, can be used for both classification and regression, while others can only solve one type of problem.</p>
</li>
</ol>
<h2 id="basic-practice">Basic Practice</h2>
<h3 id="feature-engineering">Feature Engineering</h3>
<ol>
<li>
<p>Feature engineering: the problem of transforming raw data into a dataset.</p>
</li>
<li>
<p>The role of the data analyst is to create informative features with high predictive power.</p>
</li>
<li>
<p>We say a model has a low bias when it predicts the training data well.</p>
</li>
</ol>
<h4 id="one-hot-encoding-categorical---numerical">One-Hot Encoding (Categorical -&gt; Numerical)</h4>
<ol>
<li>
<p>One-hot is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0). It transforms a categorical feature into several binary ones.</p>
</li>
<li>
<p>Example: red = [1,0,0]; yellow = [0,1,0]; green = [0,0,1].</p>
</li>
<li>
<p>When the ordering matters, we can just do: {poor, decent, good, excellent} -&gt; {1, 2, 3, 4}.</p>
</li>
</ol>
<h4 id="binning-bucketing-numerical---categorical">Binning (Bucketing) (Numerical -&gt; Categorical)</h4>
<ol>
<li>Binning converts a continuous feature in to multiple binary features called bins, typically based on value range.</li>
</ol>
<h4 id="normalization">Normalization</h4>
<ol>
<li>
<p>Normalization converts a numerical value into a value typically in the interval [-1,1] or [0,1], which increases the speed of learning:</p>
<p>$$\bar x^{(j)} = \frac{x^{(j)}-min^{(j)}}{max^{(j)}-min^{(j)}}$$</p>
</li>
</ol>
<h4 id="standardization-z-score-normalization">Standardization (Z-score Normalization)</h4>
<ol>
<li>
<p>Standardization rescales the feature values so that they have the properties of a standard normal distribution with $\mu = 0$ and  $\sigma = 1$.</p>
</li>
<li>
<p>Standard scores or z-scores can be calculated as follows:</p>
<p>$$\hat x^{(j)} = \frac{x^{(j)} - \mu^{(j)}}{\sigma^{(j)}}$$</p>
</li>
<li>
<p>Normalization vs Standardization:</p>
<ul>
<li>Unsupervised learning - standardization.</li>
<li>The distribution is close to a normal distribution - standardization.</li>
<li>Have outliers - standardization (normalization will squeeze the normal values into a very small range).</li>
<li>Other cases - normalization.</li>
</ul>
</li>
</ol>
<h4 id="dealing-with-missing-features">Dealing with Missing Features</h4>
<ol>
<li>
<p>Remove the examples with missing features</p>
</li>
<li>
<p>Use a learning algorithm that can deal with missing feature values.</p>
</li>
<li>
<p>Use Data imputation (the process of replacing missing data with substituted values)</p>
<ul>
<li>Replace the missing value with an average value.</li>
<li>Replace the missing value with a value outside the normal range.</li>
<li>Replace the missing value with a value in the middle of the range.</li>
<li>Use the missing value as the target for a regression problem.</li>
<li>Increase the dimensionality by adding a binary indicator feature.</li>
</ul>
</li>
</ol>
<h3 id="learning-algorithm-selection">Learning Algorithm Selection</h3>
<ul>
<li>Explainability</li>
<li>In-memory vs. out-of-memory (batch or incremental/online)</li>
<li>Number of features and examples (neural networks for a huge number of features)</li>
<li>Categorical vs. numerical features</li>
<li>Nonlinearity of the data (SVM/linear regression/linear kernel/logistic regression of lineraly separable data)</li>
<li>Training speed (neural networks are slow to train)</li>
<li>Prediction speed
<img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/ml_map.png"
        data-srcset="/images/ml_map.png, /images/ml_map.png 1.5x, /images/ml_map.png 2x"
        data-sizes="auto"
        alt="scikit-learn cheat sheet"
        title="scikit-learn cheat sheet" /></li>
</ul>
<h3 id="three-sets">Three Sets</h3>
<ol>
<li>
<p>Three sets:</p>
<ul>
<li>Training set (95%)</li>
<li>Validation set (2.5%): choose the learning algorithm and find the best values of hyperparameters.</li>
<li>Test set (2.5%): assess the model before delivery and production.</li>
</ul>
</li>
<li>
<p>The last two are also called holdout sets, which are not used to build the model.</p>
</li>
</ol>
<h3 id="underfitting-and-overfitting">Underfitting and Overfitting</h3>
<ol>
<li>
<p>Underfitting: the model makes many mistakes on the training data - high bias.</p>
<ul>
<li>The model is too simple.</li>
<li>The features are not informative enough.</li>
</ul>
</li>
<li>
<p>Overfitting: the model predicts very well the training data but poorly the data from the holdout sets. - high variance (the error due to the sensitivity to small fluctuations in the training set) (variance is the difference in fits between data sets - consistency) (large generalization error)</p>
<ul>
<li>The model is too complex.</li>
<li>Too many features but a small number of training examples.</li>
</ul>
</li>
<li>
<p>The overfitting model learns the idiosyncrasies of the training set:</p>
<ul>
<li>the noise</li>
<li>the sampling imperfection due to the small dataset size</li>
<li>other artifacts extrinsic to the decision problem</li>
</ul>
</li>
<li>
<p>Solutions for overfitting:</p>
<ul>
<li>Try a simply model</li>
<li>Reduce the dimensionality</li>
<li>Add more training data</li>
<li>Regularization - the most widely used approach</li>
</ul>
</li>
</ol>
<h3 id="regularization">Regularization</h3>
<ol>
<li>
<p>Regularization forces the learning algorithm to build a less complex model, which leads to a slightly higher bias but a much lower variance - the bias-variance tradeoff.</p>
</li>
<li>
<p>The two most widely used are L1 and L2 regularization, which add a penalizing term whose value is higher when the model is more complex.</p>
</li>
<li>
<p>For linear regression, L1 looks like this ($|\mathbf{w}| = \sum_{i=1}^D|w^{(j)}|$ and C is a hyperparameter), which tries to set most $w^{(j)}$ to value small values or zero (|| means abs here):</p>
<p>$$\min_{\mathbf{w},b} \left[C|\mathbf{w}| + \frac{1}{N}\sum_{i=1&hellip;N}(f_{\mathbf{w},b}(\mathbf{x}_i)-y_i)^2\right]$$</p>
</li>
<li>
<p>For linear regression, L2 looks like this (($||\mathbf{w}||^2 = \sum_{i=1}^D(w^{(j)})^2$):</p>
<p>$$\min_{\mathbf{w},b} \left[C||\mathbf{w}||^2 + \frac{1}{N}\sum_{i=1&hellip;N}(f_{\mathbf{w},b}(\mathbf{x}_i)-y_i)^2\right]$$</p>
</li>
<li>
<p>L1 produces a sparse model with most parameters equal to zero if C is large enough, which makes feature selection that decides which features are essential and increases explainability. L2 gives better results if our only goal is to maximize the performance on holdout sets. Plus L2 is differentiable - graident descent.</p>
</li>
<li>
<p>L1 and L2 are the special cases for elastic net regularization. L1 - lasso; L2 - ridge regularization.</p>
</li>
</ol>
<h4 id="shrinkage-ridge-regression">Shrinkage: Ridge Regression</h4>
<ol>
<li>
<p>Motivation: linear and basis expansion regression can lead to &ldquo;non-full&rdquo; column rank.</p>
</li>
<li>
<p>So we limit the size of coefficients (shrinkage) (a larger $\lambda$ means stronger shrinkage. The value is optimized by cross validation.):</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/ridge_regression.png"
        data-srcset="/images/ridge_regression.png, /images/ridge_regression.png 1.5x, /images/ridge_regression.png 2x"
        data-sizes="auto"
        alt="ridge regression"
        title="ridge regression" />
<img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/ridge_regression_estimator.png"
        data-srcset="/images/ridge_regression_estimator.png, /images/ridge_regression_estimator.png 1.5x, /images/ridge_regression_estimator.png 2x"
        data-sizes="auto"
        alt="ridge regression estimator"
        title="ridge regression estimator" />
<img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/kernel_ridge_regression.png"
        data-srcset="/images/kernel_ridge_regression.png, /images/kernel_ridge_regression.png 1.5x, /images/kernel_ridge_regression.png 2x"
        data-sizes="auto"
        alt="kernel ridge regression"
        title="kernel ridge regression" />
<img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/kernel_ridge_regression_algorithm.png"
        data-srcset="/images/kernel_ridge_regression_algorithm.png, /images/kernel_ridge_regression_algorithm.png 1.5x, /images/kernel_ridge_regression_algorithm.png 2x"
        data-sizes="auto"
        alt="kernel ridge regression algorithm"
        title="kernel ridge regression algorithm" /></p>
</li>
</ol>
<h3 id="model-performance-assessment">Model Performance Assessment</h3>
<ol>
<li>
<p>For regression, we first compare our model with the mean model, which always predicts the average of the labels in the training data. If the fit or our model is better than the mean model, we compare the MSE (mean squared error) for the training and test data. If the MSE for the test data is substantially higher, it is a sign of overfitting.</p>
</li>
<li>
<p>For classification, we have the following metrics and tools:</p>
</li>
</ol>
<h4 id="confusion-matrix">Confusion Matrix</h4>
<ol>
<li>
<p>The confusion matrix is a table that summarizes how successful the classification model at predicting examples belonging to various classes.</p>
</li>
<li>
<p>Take a binary classification problem as an example:</p>
<table>
<thead>
<tr>
<th></th>
<th>spam (predicted)</th>
<th>not_spam (predicted)</th>
</tr>
</thead>
<tbody>
<tr>
<td>spam (actual)</td>
<td>23 (TP)</td>
<td>1 (FN)</td>
</tr>
<tr>
<td>not_spam (actual)</td>
<td>12 (FP)</td>
<td>556 (TN)</td>
</tr>
</tbody>
</table>
<p>TP - True Positive; FN - False Negative;</p>
<p>FP - False Positive; TN - True Negative;</p>
</li>
</ol>
<h4 id="precisionrecall">Precision/Recall</h4>
<ol>
<li>
<p>Precision = $\frac{TP}{TP+FP}$ (TP/Positive Predictions)</p>
</li>
<li>
<p>Recall = $\frac{TP}{TP+FN}$ (TP/Positive Examples)</p>
</li>
</ol>
<h4 id="accuracy">Accuracy</h4>
<ol>
<li>Accuracy = $\frac{TP+TN}{TP+TN+FP+FN}$</li>
</ol>
<h4 id="cost-sensitive-accuracy">Cost-Sensitive Accuracy</h4>
<ol>
<li>When the classes have different importance, assign cost for FP and FN then multiply before calculating the accuracy.</li>
</ol>
<h4 id="area-under-the-roc-curve-auc">Area under the ROC Curve (AUC)</h4>
<ol>
<li>
<p>ROC (Receiver Operating Characteristic) curve uses TPR (True Positive Rate &ndash; recall) and FPR (False Positive Rate &ndash; FP/FP+TN).</p>
</li>
<li>
<p>The higher the area under the ROC curve (AUC), the better the classifier.</p>
</li>
</ol>
<h3 id="hyperparameter-tuning">Hyperparameter Tuning</h3>
<ol>
<li>
<p>Grid search: use continuous parameters in the logarithmic scale to generate all the combinations.</p>
</li>
<li>
<p>Some other techniques are Random Search (provide statistical distribution) and Bayesian (based on past evaluations).</p>
</li>
</ol>
<h4 id="training-error">Training Error</h4>
<ol>
<li>
<p>Definition ($\tau$ is the training set):</p>
<p>$$TE(\hat f, \tau) = \frac{1}{N}\sum_{i=1}^NL(y_i,\hat f(x_i))$$</p>
</li>
</ol>
<h4 id="generalizationtest-error">Generalization/Test Error</h4>
<ol>
<li>
<p>Definition ($\tau$ is a sample of the random variable T):</p>
<p>$$
\begin{aligned} GE(\hat f, \tau) &amp;= E_{X,Y}(L(Y, \hat f_T(X))|T=\tau) \\ &amp;= \int_{R_Y}\int_{R_X}L(y, \hat f_T(x))\rho(x,y|\tau)dxdy \\ &amp;= \int_{R_Y}\int_{R_X}L(y, \hat f_T(x))\frac{\rho(x,y,\tau)}{\rho_\tau\tau}dxdy \end{aligned}
$$</p>
</li>
<li>
<p>Expected generalization/test error</p>
<p>$$
\begin{aligned} EGE(\hat f, \tau) &amp;= E_T(E_{X,Y}(L(Y, \hat f_T(X))|T=\tau)) \\ &amp;= \int_{R_T}(\int_{R_Y}\int_{R_X}L(y, \hat f_T(x))\rho(x,y|\tau)dxdy)\rho(\tau)d\tau \\ &amp;= \int_{R_T}\int_{R_Y}\int_{R_X}L(y, \hat f_T(x))\rho(x,y,\tau) dxdyd\tau \end{aligned}
$$</p>
</li>
</ol>
<h4 id="validation-set-error-estimation">Validation Set Error Estimation</h4>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/validation_set.png"
        data-srcset="/images/validation_set.png, /images/validation_set.png 1.5x, /images/validation_set.png 2x"
        data-sizes="auto"
        alt="validation set"
        title="validation set" /></p>
<h4 id="cross-validation">Cross-Validation</h4>
<ol>
<li>
<p>When we have few training examples, it could be prohibitive to have both validation and test set. So we can split the data into a training and a test set and use cross-validation on the training set to simulate a validation set.</p>
</li>
<li>
<p>Each subset of the training set is called a fold. Typically, a five-fold cross-validation is used in practice. We would train five models (use $F_2 F_3 F_4 F_5$ for model 1 and so on) and compute the value of the metric of interest on each validation set, from $F_1$ to $F_5$ and get the average score.</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/cross_validation.png"
        data-srcset="/images/cross_validation.png, /images/cross_validation.png 1.5x, /images/cross_validation.png 2x"
        data-sizes="auto"
        alt="cross validation"
        title="cross validation" /></p>
</li>
<li>
<p>Leave-one-out cross validation:</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/leave_one_out_validation.png"
        data-srcset="/images/leave_one_out_validation.png, /images/leave_one_out_validation.png 1.5x, /images/leave_one_out_validation.png 2x"
        data-sizes="auto"
        alt="leave-one-out cross validation"
        title="leave-one-out cross validation" /></p>
</li>
</ol>
<h2 id="neural-networks-and-deep-learning">Neural Networks and Deep Learning</h2>
<h3 id="neural-networks">Neural Networks</h3>
<ol>
<li>The function $f_{NN}$ is a nested function. So for a 3-layer neural network that returns a scalar:</li>
</ol>
<p>$$y=f_{NN}(\mathbf{x}) = f_3(\mathbf{f}_2(\mathbf{f}_1(\mathbf{x})))$$</p>
<ol>
<li>$\mathbf{f}_1$ and $\mathbf{f}_2$ are vector functions of the following form ($\mathbf{g}_l$ is the activation function - a fixed and chosen nonlinear function; $\mathbf{W}_l$ (a matrix) and $\mathbf{b}_l$ (a vector) are learned using gradient descent):</li>
</ol>
<p>$$\mathbf{f}_l(\mathbf{z}) = \mathbf{g}_l(\mathbf{W}_l\mathbf{z} + \mathbf{b}_l)$$</p>
<h4 id="multilayer-perceptron-vanilla-neural-network">Multilayer Perceptron (Vanilla Neural Network)</h4>
<ol>
<li>
<p>A multiplayer perceptron (MLP) is a class of a feed-forward neural network (FFNN). It has a fully-connected architecture.</p>
</li>
<li>
<p>In each rectangle unit:</p>
<ol>
<li>All inputs of the unit are joined together to form an input vector.</li>
<li>The unit applies a linear transformation.</li>
<li>The unit applies an activation function.</li>
</ol>
</li>
<li>
<p>An example (each unit&rsquo;s parameters $w_{l,u}$ and $b_{l,u}$):</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/mlp.png"
        data-srcset="/images/mlp.png, /images/mlp.png 1.5x, /images/mlp.png 2x"
        data-sizes="auto"
        alt="MLP"
        title="MLP" /></p>
</li>
<li>
<p>A more rigorous definition:</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/multilayer_perceptron.png"
        data-srcset="/images/multilayer_perceptron.png, /images/multilayer_perceptron.png 1.5x, /images/multilayer_perceptron.png 2x"
        data-sizes="auto"
        alt="multilayer perceptron"
        title="multilayer perceptron" /></p>
</li>
<li>
<p>We can do regression/classification by optimizing the weight $\omega \ \alpha$.</p>
<ul>
<li>Highly non-linear, non-convex -&gt; many local minima.</li>
<li>$\omega \ \alpha$ often strongly underdetermined.</li>
<li>No closed form formula -&gt; use gradient descent.</li>
</ul>
</li>
</ol>
<h4 id="feed-forward-neural-network-architecture">Feed-Forward Neural Network Architecture</h4>
<ol>
<li>
<p>If we want to solve a regression or a classification problem, the last layer usually contains only one unit. If $g_{last}$ is linear, then the neural net is a regression model. If $g_{last}$ is a logistic function, it&rsquo;s a binary classification model.</p>
</li>
<li>
<p>Any differentiable function can be chosen as $g_{l,u}$. And primary reason of such a nonlinear component is to allow the neural net to approximate nonlinear functions.</p>
</li>
<li>
<p>Popular choices for $g$ are the logistic function, TanH, and ReLU (rectified linear unit function):</p>
<p>$$tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$</p>
<p>$$relu(z) = 0 \ \ if \ z &lt; 0$$</p>
<p>$$relu(z)= z \ \ otherwise$$</p>
</li>
</ol>
<h3 id="deep-learning">Deep Learning</h3>
<ol>
<li>
<p>Deep learning refers to training neural networks with more than two non-ouput layers.</p>
</li>
<li>
<p>The two biggest challenges:</p>
<ul>
<li>Exploding gradient - use gradient clipping or $L_1$ $L_2$ regularization</li>
<li>Vanishing gradient - use ReLU, LSTM, and skip connections (in residual neural networks)</li>
</ul>
</li>
<li>
<p>Backpropagation is an efficient algorithm for computing gradients on neural networks using the chain rule. (It&rsquo;s used to update the parameters.)</p>
</li>
<li>
<p>For overfitting:</p>
<ul>
<li>Early stopping in training.</li>
<li>$L_2$ regularization.</li>
<li>Add dropout layer.</li>
</ul>
</li>
</ol>
<h4 id="convolutional-neural-network">Convolutional Neural Network</h4>
<ol>
<li>
<p>When our training examples are images, the input is very high-dimensional (each picture is a feature). If we use MLP, the optimization problem is likely to become intractable.</p>
</li>
<li>
<p>A convolutional neural network (CNN) is a special kind of FFNN that significantly reduces the number of parameters. Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.</p>
</li>
<li>
<p>We use the moving window approach and then train multiple smaller regression models at once, each small regression model receiving a square patch as input. The goal of each small regression model is to detect a specific kind of pattern in the input patch. To detect some pattern, a small regression model has to learn the parameters of a matrix $\mathbf{F}$ (filter).</p>
</li>
<li>
<p>For example, if we have the following matrix $\mathbf{P}$ (patch) and $\mathbf{F}$, we can calculate the convolution of them. And the value is higher the more similar $\mathbf{F}$ is to $\mathbf{P}$. (The patch represents a pattern that looks like a cross.)</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/convolution.png"
        data-srcset="/images/convolution.png, /images/convolution.png 1.5x, /images/convolution.png 2x"
        data-sizes="auto"
        alt="convolution"
        title="convolution" /></p>
</li>
<li>
<p>For convenience, there&rsquo;s also a bias parameter b associated with each filter which is added to the result to a convolution.</p>
</li>
<li>
<p>One layer of a CNN consists of multiple convolution filters. Each filter of the first layer slides - or convolves - across the input image, left to right, top to bottom.</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/convolving.png"
        data-srcset="/images/convolving.png, /images/convolving.png 1.5x, /images/convolving.png 2x"
        data-sizes="auto"
        alt="convolving"
        title="convolving" /></p>
</li>
<li>
<p>The filters and bias values are trainable parameters and are optimized using gradient descent with backpropagation.</p>
</li>
<li>
<p>The subsequent layer treats the output of the preceding layer as a collection of image matrices. Such a collection is called a volume.</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/volume.png"
        data-srcset="/images/volume.png, /images/volume.png 1.5x, /images/volume.png 2x"
        data-sizes="auto"
        alt="volume"
        title="volume" /></p>
</li>
<li>
<p>Stride is the step size of the moving window. We can also use paddings.</p>
</li>
<li>
<p>Pooling is a layer that applies a fixed operator, usually either max or average instead of applying a trainable filter. It increases the accuracy and improves the training speed by reducing the number of parameters. (The one below uses max.)</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/pooling.png"
        data-srcset="/images/pooling.png, /images/pooling.png 1.5x, /images/pooling.png 2x"
        data-sizes="auto"
        alt="pooling"
        title="pooling" /></p>
</li>
</ol>
<h4 id="recurrent-neural-network">Recurrent Neural Network</h4>
<ol>
<li>RNN is the neural network whose connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior.</li>
</ol>
<h2 id="problems-and-solutions">Problems and Solutions</h2>
<h3 id="basis-expansions">Basis Expansions</h3>
<ol>
<li>
<p>Basis expansion extends the linear model to a non-linear model. We achieve this by introducing basis functions $\phi_m (X)$.</p>
</li>
<li>
<p>Definition:</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/basis_expansion.png"
        data-srcset="/images/basis_expansion.png, /images/basis_expansion.png 1.5x, /images/basis_expansion.png 2x"
        data-sizes="auto"
        alt="basis expansion"
        title="basis expansion" /></p>
</li>
<li>
<p>For quadratic polynomial regression, our basis functions will be: $\phi_1(x)=1, \ \phi_2(x)=x, \ \phi_3(x)=x^2$</p>
</li>
</ol>
<h4 id="regression-with-gaussian-kernel">Regression with Gaussian Kernel</h4>
<ol>
<li>
<p>The Gaussian kernel ($\sigma^2$ is a scaler parameter, i.e. the variance or kernel width):</p>
<p>$$k(x,x&rsquo;)=e^{-\frac{||x-x'||^2}{\sigma^2}}$$</p>
</li>
<li>
<p>Regression for kernel-based model:</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/kernel_based_regression.png"
        data-srcset="/images/kernel_based_regression.png, /images/kernel_based_regression.png 1.5x, /images/kernel_based_regression.png 2x"
        data-sizes="auto"
        alt="kernel-based regression"
        title="kernel-based regression" />
<img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/kernel_algorithm.png"
        data-srcset="/images/kernel_algorithm.png, /images/kernel_algorithm.png 1.5x, /images/kernel_algorithm.png 2x"
        data-sizes="auto"
        alt="kernel algorithm"
        title="kernel algorithm" /></p>
</li>
</ol>
<h3 id="kernel-regression">Kernel Regression</h3>
<ol>
<li>
<p>Kernel regression is a non-parametric method, which means there are no parameters to learn. The model is based on the data itself (like in kNN).</p>
</li>
<li>
<p>The simplest kernel regression looks like this (the function $k(\cdot)$ is called a kernel, which plays the role of a similarity function - $w_i$ is higher when x is similar to $x_i$):</p>
<p>$$f(x)=\frac{1}{N}\sum_{i=1}^{N}w_iy_i, \ where \ w_i = \frac{Nk(\frac{x_i - x}{b})}{\sum_{l=1}^Nk(\frac{x_l - x}{b})}$$</p>
</li>
<li>
<p>Kernels can have different forms, the most frequently used is the Gaussian kernel:</p>
<p>$$k(z)=\frac{1}{\sqrt{2\pi}}exp(\frac{-z^2}{2})$$</p>
</li>
</ol>
<h3 id="classification">Classification</h3>
<ol>
<li>
<p>Decision boundary: the set of points at which the probability of x to be in either of the classes is equal:</p>
<p>$$\{x \in \mathbb{R}_X|p(1|x)=p(2|x)\}$$</p>
</li>
<li>
<p>The classification is linear if the decision boundaries are linear.</p>
</li>
<li>
<p>Classification by linear regression of indicator matrix:</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/classification_by_linear_regression.png"
        data-srcset="/images/classification_by_linear_regression.png, /images/classification_by_linear_regression.png 1.5x, /images/classification_by_linear_regression.png 2x"
        data-sizes="auto"
        alt="classification by linear regression"
        title="classification by linear regression" /></p>
</li>
</ol>
<h4 id="linear-discriminant-analysis">Linear Discriminant Analysis</h4>
<ol>
<li>
<p>Linear Discriminant analysis (LDA) is a method used to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.</p>
</li>
<li>
<p>The mathematical definition and the algorithm:</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/LDA.png"
        data-srcset="/images/LDA.png, /images/LDA.png 1.5x, /images/LDA.png 2x"
        data-sizes="auto"
        alt="LDA"
        title="LDA" />
<img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/LDA_algorithm.png"
        data-srcset="/images/LDA_algorithm.png, /images/LDA_algorithm.png 1.5x, /images/LDA_algorithm.png 2x"
        data-sizes="auto"
        alt="LDA algorithm"
        title="LDA algorithm" /></p>
</li>
<li>
<p>LDA ia linear predictor.</p>
</li>
<li>
<p>A non-linear classification predictor can be obtained if we use $N(\mu_g, \Sigma_g)$ instead of $N(\mu_g, \Sigma)$, i.e. a class dependent covariance.</p>
</li>
</ol>
<h3 id="ensemble-learning">Ensemble Learning</h3>
<ol>
<li>Ensemble learning trains a large number of low-accuracy models and then combine the predictions given by the weak models to obtain a high-accuracy meta-model.</li>
</ol>
<h4 id="boosting-and-bagging">Boosting and Bagging</h4>
<ol>
<li>
<p>Boosting: each new model would be different from the previous ones by trying to fix the errors of the previous models.</p>
</li>
<li>
<p>Bagging: create many copies of the training data (each slightly different from others). Random forest is based on bagging.</p>
</li>
</ol>
<h4 id="random-forest">Random Forest</h4>
<ol>
<li>Random forest avoids the correlation of the trees, which reduces the variance.</li>
</ol>
<h2 id="unsupervised-learning-1">Unsupervised Learning</h2>
<p>Unsupervised learning deals with problems in which data doesn&rsquo;t have labels.</p>
<h3 id="density-estimation">Density Estimation</h3>
<p>To be filled.</p>
<h3 id="clustering">Clustering</h3>
<ol>
<li>
<p>Clustering is a problem of learning to assign a label to examples by leveraging an unlabeled dataset.</p>
</li>
<li>
<p>A dissimilarity function measures the difference / lack of affinity between two observations from X. It holds that $d(x,x) = 0$ and $d(x,x&rsquo;)=d(x&rsquo;,x)$. Some examples:</p>
<ul>
<li>Squared Euclidean Distance: $d(x,x&rsquo;)=\sum_{j=1}^D(x_j-x_{j}')^2$</li>
<li>Componentwise Distance: $d(x,x&rsquo;)=\sum_{j=1}^D|x_j-x_{j}&lsquo;|$</li>
<li>Weighted Squared Euclidean Distance: $d(x,x&rsquo;)=\sum_{j=1}^Dw_j(x_j-x_{j}')^2$</li>
</ul>
</li>
<li>
<p>In combinatorial clustering, we look for an encoder $C:\{1,..,N\} \rightarrow \{1,..,K\}$ that assigns each sample $x_i$ a label or cluster $k = C(i)$ such that a given loss is minimized.</p>
</li>
<li>
<p>The canonical loss for the encoder:</p>
<p>$$L(C) = \frac{1}{2}\sum_{k=1}^K\sum_{C(i)=k}\sum_{C_(i&rsquo;)=k}d(x_i,x_{i&rsquo;})$$</p>
</li>
<li>
<p>We can use centroids ($\bar x_k = \frac{1}{N_k}\sum_{C(i)=k}x_i$) to simplify the loss function</p>
<p>$$L(C)=\sum_{k=1}^KN_k\sum_{C(i)=k}||x_i - \bar x_k||^2$$</p>
</li>
</ol>
<h4 id="k-means">K-Means</h4>
<ol>
<li>
<p>Choose k - the number of clusters (a hyperparameter which requires an educated guess).</p>
</li>
<li>
<p>Randomly put k feature vectors, called centroids, to the feature space. (The initial positions influence the final positions.)</p>
</li>
<li>
<p>Compute the distance from each example x to each centroid c using some metric, like the Euclidean distance. Assign the closest centroid to each example.</p>
</li>
<li>
<p>For each centroid, calculate the average feature vector of the examples labeled with it. The average feature vectors become the new locations of the centroids.</p>
</li>
<li>
<p>Recompute the distance from each example to each centroid, modify the assignment and repeat the procedure until the assignments don&rsquo;t change after the centroid locations were recomputed.</p>
</li>
<li>
<p>The algorithm (using iterative greedy descent):</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/k_means.png"
        data-srcset="/images/k_means.png, /images/k_means.png 1.5x, /images/k_means.png 2x"
        data-sizes="auto"
        alt="k-means"
        title="k-means" /></p>
</li>
</ol>
<h3 id="dimensionality-reduction">Dimensionality Reduction</h3>
<ol>
<li>
<p>Since modern ML algorithms can handle very high-dimensional examples, dimensionality reduction techniques are used less in practice than in the past.</p>
</li>
<li>
<p>The most frequent use case is data visualization: human can only interpret on a plot the maximum of three dimensions.</p>
</li>
<li>
<p>Another situation is that we need to build an interpretable model and are limited in the choice of algorithms. Then it removes redundant or highly correlated features and also reduces the noise in the data, which all improve the interpretability of the model.</p>
</li>
</ol>
<h4 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h4>
<ol>
<li>
<p>Example:</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/pca.png"
        data-srcset="/images/pca.png, /images/pca.png 1.5x, /images/pca.png 2x"
        data-sizes="auto"
        alt="PCA"
        title="PCA" /></p>
</li>
<li>
<p>Principal components are vectors that define a new coordinate system in which the first axis goes in the direction of the highest variance in the data. The second axis is orthogonal to the first one and goes in the direction of the second highest variance in the data. The length of the arrow reflects the variance in the direction. They form the orthogonal basis.</p>
</li>
<li>
<p>If we want to reduce the dimensionality to $D_{new} &lt; D$, we pick $D_{new}$ largest principal components and project our data points on them. For 2D, we can set $D_{new} = 1$ and get the projected orange points in the figure. To describe each orange point, we need only one coordinate instead of two: the coordinate with respect to the first principal component.</p>
</li>
<li>
<p>For data compression/ dimensionality reduction, we aim to look for a lower-dimensional representation / compression of data (lossy compression, ${W}_d \ {V}_d$ are matrices for compression and decompression respectively):</p>
<p>$$z_i = comp_d (x_i)$$
$$x_i \approx \hat x_i = decomp_d (z_i)$$
$$(\hat{V_d}, \hat{W_d}) = argmin_{W_d, {V}_d} \sum_{i=1}^N||x_i-V_d {W}_d x_i||^2$$</p>
</li>
<li>
<p>For the matrix above ($\hat{V_d}$), it&rsquo;s orthonormal and $\hat{W_d} = \hat{V_d^T}$. So we can rewrite our minimization problem (the columns of $\hat {V}_d$ are called principle components, which are ordered by importance):</p>
<p>$$\hat {V_d} = argmin_{V_d} \sum_{i=1}^N||x_i-V_d V_d^T x_i||^2$$</p>
</li>
<li>
<p>Algorithms (svd -&gt; singular value)</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/PCA_algorithm.png"
        data-srcset="/images/PCA_algorithm.png, /images/PCA_algorithm.png 1.5x, /images/PCA_algorithm.png 2x"
        data-sizes="auto"
        alt="PCA algorithm"
        title="PCA algorithm" />
<img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/compression.png"
        data-srcset="/images/compression.png, /images/compression.png 1.5x, /images/compression.png 2x"
        data-sizes="auto"
        alt="compression"
        title="compression" />
<img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/images/decompression.png"
        data-srcset="/images/decompression.png, /images/decompression.png 1.5x, /images/decompression.png 2x"
        data-sizes="auto"
        alt="decompression"
        title="decompression" /></p>
</li>
</ol>
<h2 id="references">References</h2>
<ul>
<li><a href="https://www.goodreads.com/book/show/43190851-the-hundred-page-machine-learning-book" target="_blank" rel="noopener noreffer">The Hundred-Page Machine Learning Book</a></li>
<li><a href="https://www.goodreads.com/book/show/148009.The_Elements_of_Statistical_Learning?ac=1&amp;from_search=true&amp;qid=8GAFr2BFEO&amp;rank=1" target="_blank" rel="noopener noreffer">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</a></li>
<li><a href="https://www.goodreads.com/book/show/17397466-an-introduction-to-statistical-learning" target="_blank" rel="noopener noreffer">An Introduction to Statistical Learning: With Applications in R</a></li>
<li>Machine Learning course by <a href="https://www.jacobs-university.de/directory/zaspel" target="_blank" rel="noopener noreffer">Prof. Dr. Peter Zaspel</a> at Jacobs University Bremen</li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on May 1, 2020</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/posts/2020-05-01-Machine-Learning-Notes/index.md" target="_blank">Read Markdown</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://tillchen.com/posts/2020-05-01-Machine-Learning-Notes/" data-title="Machine Learning Notes" data-hashtags="Machine Learning,AI"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://tillchen.com/posts/2020-05-01-Machine-Learning-Notes/" data-hashtag="Machine Learning"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://tillchen.com/posts/2020-05-01-Machine-Learning-Notes/"><i class="fab fa-linkedin fa-fw"></i></a><a href="javascript:void(0);" title="Share on WhatsApp" data-sharer="whatsapp" data-url="https://tillchen.com/posts/2020-05-01-Machine-Learning-Notes/" data-title="Machine Learning Notes" data-web><i class="fab fa-whatsapp fa-fw"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://tillchen.com/posts/2020-05-01-Machine-Learning-Notes/" data-title="Machine Learning Notes"><i data-svg-src="/lib/simple-icons/icons/line.min.svg"></i></a><a href="javascript:void(0);" title="Share on " data-sharer="weibo" data-url="https://tillchen.com/posts/2020-05-01-Machine-Learning-Notes/" data-title="Machine Learning Notes"><i class="fab fa-weibo fa-fw"></i></a><a href="javascript:void(0);" title="Share on Evernote" data-sharer="evernote" data-url="https://tillchen.com/posts/2020-05-01-Machine-Learning-Notes/" data-title="Machine Learning Notes"><i class="fab fa-evernote fa-fw"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/Machine-Learning/">Machine Learning</a>,&nbsp;<a href="/tags/AI/">AI</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/2020-4-10-Algorithms-and-Data-Structures/" class="prev" rel="prev" title="Algorithms and Data Structures"><i class="fas fa-angle-left fa-fw"></i>Algorithms and Data Structures</a>
            <a href="/posts/2020-05-21-Software-Engineering-Notes/" class="next" rel="next" title="Software Engineering Notes">Software Engineering Notes<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
<div id="comments"><div id="disqus_thread" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://disqus.com/?ref_noscript">Disqus</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.72.0">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.9"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2020</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://tillchen.com" target="_blank">Tianyao Chen</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.4710034e669c7ff17f823f9ba12cf8a36582d65b007f79cbc4a3c11d7db2e4ca.css" integrity="sha256-RxADTmacf/F/gj&#43;boSz4o2WC1lsAf3nLxKPBHX2y5Mo="><link rel="stylesheet" href="/lib/katex/copy-tex.min.bf9ff4137fec38f6255419e142d0883c9c52090885d746f80eee12b273d9b3e0.css" integrity="sha256-v5/0E3/sOPYlVBnhQtCIPJxSCQiF10b4Du4SsnPZs&#43;A="><script type="text/javascript" src="https://tillchen.disqus.com/embed.js" defer></script><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.bcff85fb5e00d68802850b393ac7792c997f722f536f38e26638c46dca8e5eb6.js" integrity="sha256-vP&#43;F&#43;14A1ogChQs5Osd5LJl/ci9TbzjiZjjEbcqOXrY="></script><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.615590a2ca2b667afa7c02ef396f5500b62e22795ddbb46448f90494605d09a5.js" integrity="sha256-YVWQosorZnr6fALvOW9VALYuInld27RkSPkElGBdCaU="></script><script type="text/javascript" src="/lib/lunr/lunr.min.df84a2d58ea594c04a3371b48d020b55ea10284c2ec636e4e331965d7313e29b.js" integrity="sha256-34Si1Y6llMBKM3G0jQILVeoQKEwuxjbk4zGWXXMT4ps="></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.876b4c12685e991d88378c1b6dd3638fd2da0c88f3c24da1ada950c1f26604e1.js" integrity="sha256-h2tMEmhemR2IN4wbbdNjj9LaDIjzwk2hralQwfJmBOE="></script><script type="text/javascript" src="/lib/twemoji/twemoji.min.e5845681ecd26aaf715f65c6b871e241c1d37b95960d5a258126a27d481ae306.js" integrity="sha256-5YRWgezSaq9xX2XGuHHiQcHTe5WWDVolgSaifUga4wY="></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.8a7739925f4c03586479852df840b7061948832a7fda30c8c812d2ea4dd4c4f2.js" integrity="sha256-inc5kl9MA1hkeYUt&#43;EC3BhlIgyp/2jDIyBLS6k3UxPI="></script><script type="text/javascript" src="/lib/sharer/sharer.min.9c88f86c7f0820287113f6236200459832693656e80d7556cc80a93dfbd45813.js" integrity="sha256-nIj4bH8IIChxE/YjYgBFmDJpNlboDXVWzICpPfvUWBM="></script><script type="text/javascript" src="/lib/katex/katex.min.17f5dd6b9f123dd7140abfb18521b3f4c036cd004f6f40121182a8865f140877.js" integrity="sha256-F/Xda58SPdcUCr&#43;xhSGz9MA2zQBPb0ASEYKohl8UCHc="></script><script type="text/javascript" src="/lib/katex/auto-render.min.f74776a677f0d2be0af0264058f928e2ba455d0b19bc985304660d922a43a6b2.js" integrity="sha256-90d2pnfw0r4K8CZAWPko4rpFXQsZvJhTBGYNkipDprI="></script><script type="text/javascript" src="/lib/katex/copy-tex.min.2ab2237329021bc443986c8327f6e61357fb68a54e5d233d224023718c02207d.js" integrity="sha256-KrIjcykCG8RDmGyDJ/bmE1f7aKVOXSM9IkAjcYwCIH0="></script><script type="text/javascript" src="/lib/katex/mhchem.min.5cea356d6025c5a2f18c454c83ec5674dbb04fab1cd1d75569e77788c6b6f888.js" integrity="sha256-XOo1bWAlxaLxjEVMg&#43;xWdNuwT6sc0ddVaed3iMa2&#43;Ig="></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"$$","right":"$$"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"},{"display":false,"left":"$","right":"$"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"lunr"},"twemoji":true};</script><script type="text/javascript" src="/js/theme.min.b6a719a7cf033e3fc0580737dd792b949c3decc876163026fb8e0973104c2996.js" integrity="sha256-tqcZp88DPj/AWAc33XkrlJw97Mh2FjAm&#43;44JcxBMKZY="></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-139593886-1', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-139593886-1" async></script></body>
</html>
